{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 번역자: 고려대학교 수학과(12) 최시현 (고려대학교 정보대학 이상근 교수님 데이터 인텔리젼스 연구실 인턴)\n",
    "[CS231n](http://cs231n.stanford.edu/)를 번역하였습니다. <br />\n",
    "문제가 될시에는 자진 삭제하겠습니다. <br /> \n",
    "오류 수정 및 내용에 대한 기타 문의 사항은 passkmla@naver.com으로 연락주시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ConvNet이 학습한 것을 시각화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "뉴럴 네트워크에서 학습된 특징들은 해석하기 힘들다는 비판에 대한 대응으로, <br /> \n",
    "ConvNet을 이해하고 시각화하기 위한 몇가지 접근법들이 개발되고 있다. <br />\n",
    "이 장에서 이것과 관련된 접근법들에 대해 간단히 알아볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the activations and first-layer weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 직접적인 시각화 기법은 forward pass동안 네트워크의 activations를 보여주는 것이다. <br />\n",
    "ReLU 네트워크에서, activations는 상대적으로 blobby and dense한 것으로 보이기 시작한다. <br />\n",
    "그러다가 학습 프로세스가 진행됨에 따라 더 희소해지고 지역화된다. <br /> \n",
    "이 시각화에서 쉽게 알아차릴 수 있는 하나의 pitfall은 많은 서로 다른 입력값에 대하여 몇몇 activation 맵은 모두 0의 값을 갖는다는 것이다. <br />\n",
    "이는 dead 필터를 나타내는 것이고, 높은 학습률의 징조일 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "첫 번째 Conv 레이어의 typical-looking activations(왼쪽)와, 고양이 그림을 보고 있는 학습된 AlexNet의 5번째 Conv 레이어 <br /> \n",
    "모든 상자는 어떤 필터에 상응하는 activation 맵을 보여준다. <br /> \n",
    "activations는 희소(대부분의 값이 0, 검은색으로 시각화 됨)이고 대부분 지역적이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conv/FC 필터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 번째로 보통 사용되는 전략은 가중치들을 시각화하는 것이다. 원본 필섹 자료를 직접보는 첫 번째 Conv 레이어는 대게 쉽게 해석가능하다. <br />\n",
    "네트워크에서 더 깊은 필터의 가중치들을 보는 것 또한 가능한데 이 가중치들은 시각화 하기에 유용하다. <br /> \n",
    "왜냐하면 잘 학습된 네트워크는 대게 nice display를 나타내고, 노이즈 패턴 없는 smooth한 필터를 보여주기 때문이다. <br />\n",
    "노이즈 패턴은 충분히 학습되지 않은 네트워크의 지표가 될 수있고, <br /> \n",
    "과적합의 원인이 되는 너무 낮은 정규화(regularization) 강도를 나타내는 지표일 수 있다. <br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 AlexNet에서 첫 번째 Conv 레이어(왼쪽)와 2번째 Conv 레이어(오른쪽)에서 일반적으로 보이는 필터. <br />\n",
    "첫 번째 레이어 가중치들은 매우 좋고 부드럽다. 이는 잘 수렴된 네트워크를 나타낸다. <br /> \n",
    "AlexNet은 processing의 두 가지 나누어진 흐름을 포함하고 그 결과로 색/gray-scale features들이 군집한다. <br />\n",
    "하나의 흐름은 high-frequency gray-scale을 발달시키고, 다른 하나는 low-frequency color feature를 발달시킨다는 것이다. <br />\n",
    "두 번째 Conv 레이어의 가중치들은 해석가능하지는 않지만, 이것이 여전히 부드럽고, 잘 형성되어 있고, 노이즈 패턴이 없는 것은 명백하다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieving images that maximally activate a neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 시각화 기법으로는, 많은 이미지들을 네트워크를 통해 feed하고 뉴런을 최대로 activate시키는 이미지를 계속 추적하는 것이다. <br />\n",
    "Receptive field에서 뉴런이 찾고 있는 것이 무엇인지 이해하기 위해 이미지를 시각화 할 수 있다. <br />\n",
    "One such visualization (among others) is shown in [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524) by Ross Girshick et al.:\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AlexNet의 POOL5(5th pool layer)를 가장 잘 activating하는 이미지. <br />\n",
    "Activation 값과 특정 뉴런의 receptive field를 흰색으로 나타내었다. <br /> \n",
    "(특히, POOL5의 뉴런들은 상대적으로 입력 이미지의 큰 부분을 차지하는 함수이다) <br />\n",
    "이것은 어떤 뉴런이 상체, 텍스트같은 특정 하이라이트에 반응하는 것으로 보일 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 접근의 한가지 문제는 ReLU 뉴런 스스로가 어떤 의미론적인 의미를 필연적으로 필요로 하는 것은 아니라는 점이다. <br />\n",
    "오히려, multiple ReLU 뉴런들을 이미지 patches를 나타내는 어떤 공간의 기저 벡터로 생각하는 것이 적절하다. <br />\n",
    "즉 시각화는 필터의 가중치들에 대응하는 임의의 축을 따라 the patches at the edge of the cloud of representations를 보여준다. <br />\n",
    "또한 ConvNet에서는 뉴런이 입력 공간에 대하여 선형적으로 작동하고 따라서 그 공간에 대한 어떤 임의의 회전도 <br /> \n",
    "조종 불가능 하다는 사실을 알 수 있다. <br />\n",
    "This point was further argued in [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199) by Szegedy et al., where they perform a similar visualization along arbitrary directions in the representation space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding the codes with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet은 점진적으로 어떤 이미지를 선형 분류기에 의해 분리가능한 클래스에 대한 표현으로 변화시키는 것으로 해석할 수 있다. <br />\n",
    "공간의 위상에 대한 rough한 아이디어를 가지고 이미지를 2차원으로 embeding한다. <br />\n",
    "여기서 이미지들의 저차원 표현이 고차원 표현에 대하여 대략 비슷한 거리의 근사를 가진다. <br />\n",
    "쌍으로 이루어진 점들의 거리를 유지시키면서 고차원을 저차원으로 embeding하는 intuition을 발전시키는 많은 embeding 방법들이 있다. <br />\n",
    "이들 중에 [t-SNE](http://lvdmaaten.github.io/tsne/)는 시각적으로 만족스러운 결과를 일관되게 보여주는 가장 잘 알려진 방법이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeding을 하기 위해서, 이미지들의 집합을 가지고 CNN 코드를 추출하기 위해 ConvNet을 사용한다. <br />\n",
    "(e.g. AlexNet에서 ReLU의 비선형성을 포함하는 분류기 바로 직전의 4096 차원의 벡터) <br />\n",
    "이것들을 t-SNE에 연결하여 각각의 이미지에 대해 2차원 벡터를 얻을 수 있다. <br />\n",
    "해당 이미지들은 격자에서 시각화 될 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN 코드에 기반한 t-SNE embeding 이미지들의 집합 <br />\n",
    "CNN 표현 공간에서 서로 가까이 있는 이미지들은 그들을 유사하게 보고있음을 의미한다. <br />\n",
    "이 유사성은 픽셀과 색 기반 보다는 더 클래스 기반이고 의미론적이다. <br />\n",
    "For more details on how this visualization was produced the associated code, and more related visualizations at different scales refer to [t-SNE visualization of CNN codes.](http://cs.stanford.edu/people/karpathy/cnnembed/)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이미지의 가려진 부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet이 이미지를 개로 분류한다고 가정하자. <br />\n",
    "실전에서 어떻게 여러 다른 종류의 객체나 배경에서 나오는 맥락적 단서에 반대하여 이미지에서 개를 선택할 것을 확신할 수 있을까? <br />\n",
    "이미지 분류기의 예측에서 이미지의 부분을 조사하는 한가지 방법은 <br /> \n",
    "가려진 객체의 위치의 함수로서 관심있는 클래스(e.g. dog class)의 확률을 나타내는 것이다. <br />\n",
    "즉, 이미지의 patch를 전부 0로 설정하고,이미지의 영역에 대하여 반복하여 클래스의 확률을 본다. <br />\n",
    "이를 2차원 히트 맵으로 확률을 시각화 할 수 있다. <br />\n",
    "This approach has been used in Matthew Zeiler’s [Visualizing and Understanding Convolutional Networks:](https://arxiv.org/abs/1311.2901)\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "세개의 입력 이미지(위) <br />\n",
    "가려진 부분은 회색으로 표시되었다. 가려진 부분을 이미지에 대해 움직이면서 올바른 클래스일 확률을 기록하고 이것을 히트맵으로 시각화한다. <br />\n",
    "(각 이미지 아래에 보여지는 것처럼)<br /> \n",
    "예를 들어, 가장 왼쪽 이미지에서 포메라니안일 확률은 가려진 부분이 개의 얼굴일 때 급락하는 것을 볼 수 있다. <br />\n",
    "따라서 개의 얼굴이 높은 분류 스코어를 위한 주요 원인임을 확신하게 해준다. <br />\n",
    "이미지의 다른 부분을 0으로 만드는 것은 상대적으로 무시할만한 영향을 가진다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the data gradient and friends\n",
    "\n",
    "#### Data Gradient.\n",
    "\n",
    "[Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](https://arxiv.org/abs/1312.6034)\n",
    "\n",
    "#### DeconvNet.\n",
    "\n",
    "[Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)\n",
    "\n",
    "#### Guided Backpropagation.\n",
    "\n",
    "[Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806)\n",
    "\n",
    "#### Reconstructing original images based on CNN Codes\n",
    "\n",
    "[Understanding Deep Image Representations by Inverting Them](https://arxiv.org/abs/1412.0035)\n",
    "\n",
    "#### How much spatial information is preserved?\n",
    "\n",
    "[Do ConvNets Learn Correspondence?](http://papers.nips.cc/paper/5420-do-convnets-learn-correspondence.pdf) (tldr: yes)\n",
    "\n",
    "#### Plotting performance as a function of image attributes\n",
    "\n",
    "[ImageNet Large Scale Visual Recognition Challenge](https://arxiv.org/abs/1409.0575)\n",
    "\n",
    "#### Fooling ConvNets\n",
    "\n",
    "[Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572)\n",
    "\n",
    "#### Comparing ConvNets to Human labelers\n",
    "\n",
    "[What I learned from competing against a ConvNet on ImageNet](http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
