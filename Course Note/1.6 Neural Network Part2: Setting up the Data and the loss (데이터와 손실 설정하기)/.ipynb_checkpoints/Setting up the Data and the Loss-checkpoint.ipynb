{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 번역자: 고려대학교 수학과(12) 최시현\n",
    "[AiKorea 번역 프로젝트](https://github.com/aikorea/cs231n)를 이어받아서 진행하였습니다. 기존 AiKorea에서 이미 번역되어있는 CourseNote의 경우 유사한 부분이 많습니다. <br /> \n",
    "(깨진 삽화를 추가하거나 개인적으로 번역이 매끄럽지 못하다 생각하는 부분은 수정하였습니다.) <br />\n",
    "문제가 될시에는 자진 삭제하겠습니다. 오류 수정 및 내용에 대한 기타 문의 사항은 passkmla@naver.com으로 연락주시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 및 모델 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞 장에서 내적 및 비선형성의 연산을 순차적으로 수행하는 뉴런 모델과 이러한 뉴런들의 다층 구조로 구성된 뉴럴 네트워크에 대해서 소개하였다. <br />\n",
    "뉴럴 네트워크 모델은 선형 변환 결과를 비선형성 변환에 적용하는 과정이 연속적으로 발생하게 되고 <br />\n",
    "따라서 선형 분류 부분에서 소개한 선형 변환을 확장한 새로운 형태의 스코어 함수 정의를 필요로 한다. <br />\n",
    "이번 장에서는 데이터 전처리, 파라미터 초기화, 손실 함수등에 대해 소개한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 행렬 X에 대해서 일반적으로 아래의 3가지 전처리 방법을 사용한다. <br />\n",
    "여기서 데이터 X는 D차원의 데이터 벡터 N개로 이루어진 [N X D]행렬로 가정한다. <br />\n",
    "(N은 데이터의 수, D는 차원)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균 차감"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 흔하게 사용되는 데이터 전처리 기법이다. 데이터의 모든 feature 각각에 대해서 평균값 만큼 차감하는 방법으로 기하학 관점에서 보자면 <br /> \n",
    "데이터 군집을 모든 차원에 대해서 원점으로 이동시키는 것으로 해석할 수 있다. <br />\n",
    "numpy에서는 다음과 같이 구현 가능하다. (X -= np.mean(X, axis=0)) <br />\n",
    "특히 이미지 처리에 있어서 계산의 간결성을 위해서 모든 픽셀에서 동일한 값을 차감하는 방식으로 구현한다. (예를 들어, numpy에서 X -= np.mean(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규화(normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규화(normalization)는 각 차원의 데이터가 동일한 범위내의 값을 갖도록 하는 전처리 기법을 의미한다. 일반적으로 다음의 2가지 중 하나를 선택하여 구현한다. \n",
    "- 각 데이터값을 평균만큼 차감하고 표준편차 값으로 나눈다.(X /= np.std(X, axis=0)) 이 때 각 차원에 대해서 개별적 연산을 수행한다.\n",
    "- 또 다른 기법은 각 차원에서 최소/최대값이 각각 -1/1의 값을 갖도록 정규화(Normalization) 하는 것이다. <br /> \n",
    "하지만 이 기법은 scale이 다른 feature가 대략적으로나마 동일한 비중으로 <br /> \n",
    "학습 결과에 영향을 줄 것이라는 가정하에 사용하는 것이 일반적이다. <br />\n",
    "이미지 처리에서는 각 픽셀 값이 이미 동일한 scale(0~255)를 갖고 있는 경우가 대부분이기 때문에 <br /> \n",
    "정규화(normalization) 전처리 기법을 반드시 사용해야 하는 것은 아니다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=800 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "일반적인 자료 전처리 파이프라인 <br />\n",
    "왼쪽: 원본 2차원 입력자료 <br />\n",
    "중간: 이 자료는 각 차원에서 평균을 빼서 0을 중심으로 평행이동함 <br />\n",
    "오른쪽: 각 차원은 추가적으로 그들의 표준편차로 나누어서 단위가 바뀜 <br />\n",
    "빨간 색 선은 자료의 크기를 나타냄. 중간에서는 다른 길이이지만 정규화되고 난 다음 오른쪽에서는 서로 같은 길이이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA와 Whitening"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 평균 차감 기법을 이용하여 데이터를 정규화 시킨다. 그리고 데이터 간의 상관 관계를 나타내는 공분산을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assume input data matrix X of size [N x D]\n",
    "X -= np.mean(X, axis = 0) # zero-center the data (important)\n",
    "cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "공분산 행렬에서 (i,j)값은 X행렬에서 i번째, j번째 데이터 간의 상관정도를 나타내는 값이라고 해석할 수 있다. <br />\n",
    "특히, 공분산 행렬에서 대각선 상의 값들은 X행렬의 각 데이터(row 벡터)의 분산값과 같다. <br /> \n",
    "또한 공분산 행렬은 simmetric, positive semi-difinite 성질을 갖는다. 공분산 행렬의 SVD factorization은 다음과 같이 구할 수 있는데, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U,S,V = np.linalg.svd(cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 U 행렬의 열 벡터는 아이젠 벡터, S 는 특이값의 1차원 배열이다. <br /> \n",
    "공분산은 symmetric, positive semi-definite의 성질이 있으므로 S 벡터의 각 성분은 아이젠벨류의 제곱의 값을 갖는다. <br />\n",
    "데이터 X를 고유기저에 사상시킴으로서 데이터 간의 상관 관계를 없앨 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xrot = np.dot(X, U) # decorrelate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U 행렬의 열 벡터의 norm 값은 1이고 서로 직교하는 정규 직교 성질을 갖고 있기 때문에, 기저 벡터가 됨을 알 수 있다. <br />\n",
    "따라서 고유 기저로 사상하는 것은 아이젠벡터를 새로운 축으로하여 X 데이터를 회전하는 것으로 해석할 수 있다. <br />\n",
    "위의 코드에서 Xrot 행렬의 공분산을 구하면 대각 행렬인 것을 알 수 있다. <br />\n",
    "np.linalg.svd의 장점 중 하나는 U 행렬의 열 벡터는 각 벡터에 상응하는 아이젠벨류의 내림차순으로 정렬 된다는 것이다. <br />\n",
    "따라서 처음 몇개의 벡터만 사용하여 데이터 차원을 축소하는데 사용할 수 있다. <br />\n",
    "이러한 기법을 PCA 차원 축소 기법이라고 부르기도 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 연산을 통하여, [N x D]의 크기의 X데이터를 [N X 100]크기의 데이터로 압축 할 수 있는데 데이터의 variance가 가능한 <br />\n",
    "큰 값을 갖도록 하는 100개의 차원이 선택된다. PCA-축소 기법으로 전처리 된 데이터를 선형 분류기 혹은 뉴럴 네트워크에 학습시킴으로써 <br />\n",
    "좋은 성능을 기대할 수 있을 뿐만 아니라 트레이닝 시간과 사용 메모리 용량에서도 이득을 볼 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 살펴볼 기법은 화이트닝으로 이는 기저벡터 데이터를 아이젠벨류 값으로 나누어 정규화(normalization)하는 기법이다. <br />\n",
    "화이트닝 변환의 기하학적 해석은 만약 입력 데이터가 multivariable gaussian 분포를 하면 화이트닝된 데이터의 평균은 0이고 <br />\n",
    "공분산은 단위행렬을 갖는 정규분포를 갖게된다. 화이트닝은 다음과 같이 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# whiten the data:\n",
    "# divide by the eigenvalues (which are square roots of the singular values)\n",
    "Xwhite = Xrot / np.sqrt(S + 1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 노이즈 과장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 식에서 분모가 0이 되는 것을 방지하게 위해서 ${e}^{-5}$(또는 임의의 작은 상수도 무방)를 더한 것에 주목하자. <br />\n",
    "화이트닝 기법의 단점 중 하나는 모든 차원의 데이터를 동일하게 늘리게 되는데 특히 분산값이 매우 작아 노이즈로 <br /> \n",
    "해석할 수 있는 차원의 데이터까지 포함되어 데이터 내의 노이즈 과장되는 효과가 나타난다는 것이다. <br />\n",
    "이런 경우 보통(${e}^{-5}$와 같은 작은 수가 아닌) 큰 수를 분모에 더하는 방식으로 smoothing 효과를 추가하여 <br /> 이러한 노이즈 과장 현상을 완화 할 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA / 화이트닝 <br />\n",
    "왼쪽: 원본 2차원 입력 자료 <br />\n",
    "중간: PCA 수행 후, 자료는 0이 중심이고, 자료의 공분산 행렬의 고유 벡터로 회전함. 이것은 상관관계가 없어진 자료(공분산 행렬이 직교함) <br />\n",
    "오른쪽: 각 차원은 추가적으로 고유값에 의해 단위 변환됨, 변환된 자료의 공분산 행렬은 단위 행렬 이것은 등방성, <br />\n",
    "정규분포으로 자료를 늘리거나 줄임에 해당한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10 이미지에 위에서 소개된 변환들을 적용하여 각 변환 효과를 시각화 할 수 있다. <br />\n",
    "CIFAR-10 학습 데이터는 50,000x3072크기이며 각 이미지 데이터는 3072차원을 갖는 row 벡터로 표현되어 있다. <br />\n",
    "[3072x3072] 크기를 갖는 공분산 행렬을 구하고 SVD분해(연산 시간이 비교적 오래걸림)를 한다. <br />\n",
    "연산을 통하여 구해진 아이젠벡터는 어떤 특성을 보이는지 다음 그림을 통해 살펴보자.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽: 49개 이미지의 예제 집합 <br />\n",
    "두 번째: 3027 아이젠벡터의 상위 144개 출력. 자료에서 가장 많은 분산을 갖는 상위 고유벡터들이고, <br /> \n",
    "이미지에서 더 낮은 빈도에 해당하는 것들을 볼 수 있다. <br />\n",
    "오른쪽에서 두 번째: 144개의 고유벡터를 이용해 PCA로 차원 축소된 49개 이미지. <br /> \n",
    "각각의 요소가 특정 픽셀의 위치와 채널의 밝기를 나타내는 3072 차원의 벡터로 표현된 모든 이미지 대신에 <br /> \n",
    "위에 표현된 모든 이미지는 이미지를 만들기 위해 각 고유 벡터를 더하여 구성된 각각의 요소로 이루어진 144개 차원 벡터로 표현된 것이다. <br /> \n",
    "144개의 숫자만 남겨진 이미지 정보를 시각화하기 위해, 3072개로 구성된 픽셀로 다시 회전해야한다. <br />\n",
    "U 는 회전이기 때문에, 이는 U.transpose()[:144:]를 함으로서 해낼 수 있고 그러고나서 결과로 3072개의 숫자를 이미지로 시각화 한다. <br />\n",
    "이미지들이 약간 더 번진것을 볼 수 있고, 상위 고유 벡터들이 더 적은 빈도를 포착한 것을 반영한다. 그러나 대부분의 정보가 보존된다. <br />\n",
    "오른쪽: 화이트닝 표현의 시각화. 모든 144개의 차원이 동일한 길이로 압축된 분산이 사용되었다. <br />\n",
    "화이트닝 된 144개의 숫자들은 U.transpose()[:144:]로 가 곱해져서 픽셀 기반의 이미지로 다시 회전되었다. <br />\n",
    "더 낮은 빈도(대부분의 분산을 차지하는)는 이제 무시할만 하다. 반면 높은 빈도(상대적으로 작은 분산을 차지하는)는 확대된다. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실전 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 변환 기법을 소개하기 위해 PCA/화이트닝도 함께 살펴보았지만 ConvNet에서는 이 변환을 사용하는 경우는 거의 없다. <br />\n",
    "하지만 평균 차감 기법을 통해여 zero-centered 데이터로 변환하거나 각 픽셀 값을 정규화(normalization)하는 기법은 <br /> \n",
    "일반적으로 흔하게 쓰는 전처리 기법 중에 하나이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common pitfall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전처리 기법을 적용함에 있어서 명심해야 하는 중요한 사항은 전처리를 위해 여러 통계치들은 학습 데이터만 대상으로 추출하고 <br /> 검증, 테스터 데이터에 적용해야한다. 예를 들어, 평균 차감 기법을 적용할 때 흔히 하는 실수 중에 하나는 전체 데이터를 <br /> 대상으로 평균차감 처리를 하고, 이 데이터를 학습, 검증, 테스트 데이터로 나누어 사용하는 것이다. <br /> \n",
    "올바른 방법은 학습, 검증 테스트를 위한 데이터를 먼저 나눈 후에 학습 데이터를 대상으로 평균값을 구한 후에 평균 차감 전처리를 <br />\n",
    "모든 데이터군(학습, 검증, 전처리)에 적용해야 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지 뉴럴 네트워크 구조 및 데이터 전처리 기법에 대해 알아보았다. <br />\n",
    "실제 데이터를 뉴럴 네트워크 내에서 학습시키기 전에 해야하는 작업이 있는데 바로 파라미터 초기화이다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pitfall: 0으로 초기화하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "실제로 하지 말아야하는 방식을 먼저 적용해보자. <br />\n",
    "학습된 뉴럴 네트워크에서 가중치들이 최종적으로 어떤 값으로 수렴해야 하는지 알 수 없지만 데이터 정규화(normalization) 기법을 적절하게 적용하여 <br />\n",
    "가중치들 중에 절반은 양수 값 나머지 절반은 음수값을 갖는다는 가정을 할 수 있을 것이다. <br />\n",
    "더 나아가 모든 가중치들을 0으로 초기화 함으로써 최상의 학습 결과를 얻을 것이라는 아이디어 또한 합리적인 추론으로 보일 수 있다. <br />\n",
    "하지만 이러한 방법은 명백히 잘못된 방법이라는 것이 밝혀졌다. <br /> \n",
    "왜냐하면 가중치가 0으로 초기화 된 뉴럴 네트워크 내의 모든 뉴런들은 모두 동일한 연산 결과를 낼 것이고 <br /> \n",
    "따라서 backpropagation 과정에서 동일한 그라디언트값을 얻게 될 것이고 <br /> \n",
    "결과적으로 모든 파라미터들은 동일한 값으로 업데이트 될 것이기 때문이다. <br />\n",
    "다시 말해, 모든 가중치들이 동일한 값으로 초기화 된다면 뉴런들의 비대칭성을 야기할 요소가 사라지게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 언급한 이야기를 종합하자면, 가중치들은 가능한 0에 가까운 값이 되면 좋지만 모두 동일하게 0이어서는 안된다는 것이다. <br />\n",
    "따라서 해결책으로서, 소위 대칭성 깨기라는 방법을 사용하는데 이는 0에 가까운(하지만 0이 아닌) 값으로 가중치들을 초기화시키는 방법이다. <br />\n",
    "즉, 모든 가중치들은 난수를 이용하여 고유한 값으로 초기화 함으로써 각 파라미터 값들이 서로 다른 값으로 업데이트 되고 <br />\n",
    "결과적으로 전체 뉴럴 네트워크 내에서 서로 다른 특성을 보이는 다양한 부분으로 분화될 수 있다. <br />\n",
    "하나의 가중치 행렬을 다음과 같이 구현할 수 있다. W = 0.01\\*np.random.randn(D, H) <br /> \n",
    "여기서 randn은 평균 0, 표준편차 1인 정규분포로부터 얻는 값이다. <br />\n",
    "앞의 공식에 의하면, 모든 가중치 벡터는 multi-정규분포로부터 추출된 무작위 벡터로 초기화되기 때문에 입력 공간 상에서 <br /> \n",
    "각 벡터들은(특정한 패턴 혹은 방향성 없이) 무작위의 방향성을 갖게 된다. <br /> \n",
    "정규 분포가 아닌 균일 분포로부터 추출된 값으로 가중치를 초기화해도 무방하지만 <br /> \n",
    "실제로 이 방법은 학습된 최종 성능에 미치는 영향은 미미한 것으로 알려져 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치들을 0에 가까운 작은 값으로 초기화 하는 것이 항상 좋은 성능을 보장하는 것은 아니다. 예를 들어, 아주 작은 값으로 이루어진<br />\n",
    "가중치들로 구성된 뉴럴 네트워크의 경우 backpropagation 연산 과정에서 그라디언트 또한 작은 값을 가지게 된다. <br />\n",
    "(그라디언트는 가중치의 값에 비례하기 때문) <br />\n",
    "이는 네트워크의 역방향으로 흐르며 전달되는 \"그라디언트 signal\"을 감소시키게 되고 이는 뉴럴 네트워크 학습에 있어서 문제를 야기하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calibratinf the variances 1/sqrt(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 제안한 방법의 문제점 중 하나는 무작위로 초기화된 뉴런으로부터 나온 결과의 분포는 <br /> \n",
    "입력 데이터 수에 비례하여 커지는 분산을 갖는다는 것이다. <br />\n",
    "가중치 벡터를 fan-in(입력 데이터 수)의 제곱근 값으로 나누는 연산을 통하여 뉴런 출력의 분산을 1로 정규화(normalization) 할 수 있다. <br />\n",
    "권장되는 휴리스틱 기법은 뉴런의 가중치 벡터를 다음과 같이 초기화 하는 것이다. w = np.random.randn(n) / sqrt(n) (n: 입력 수) <br />\n",
    "이 방법은 근사적으로 동일한 출력 분포를 갖게 할 뿐만 아니라 뉴럴 네트워크의 수렴률 또한 향상시키는 것으로 알려져 있다. <br />\n",
    "이는 다음의 유도 과정을 통해서 확인할 수 있다. <br /> \n",
    "가중치들을 나타내는 $W$와 입력 데이터를 나타내는 $x$의 내적연산 $s=\\sum _{ i }^{ n }{ { w }_{ i } } { x }_{ i }$가 있다고 하자. <br />\n",
    "이는 비선형 연산 이전 단계에서 뉴런의 raw activation이 되고 $s$의 분산은 다음과 같이 구할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "처음 두 단계는 [분산의 성질](https://en.wikipedia.org/wiki/Variance)을 이용하여 전개하였다. 세 번째 단계에서는 가중치들과 입력 데이터 평균이 모두 0이라고 가정하고 있다. <br /> \n",
    "($E[{ x }_{ i }]=E[{ w }_{ i }]=0$) 이는 일반적인 케이스는 아니다. <br />\n",
    "예를 들어, ReLU 유닛은 양의 평균값을 갖는다. 마지막 단계는 ${w}_{i}$, ${x}_{i}$이 모두 동일한 분포 를 갖는다고 가정한다. <br /> \n",
    "위의 유도 과정으로부터 알 수 있는 것은, $s$가 이것의 모든 입력 데이터 $x$와 동일한 분산을 갖기를 원하므로, <br /> \n",
    "초기화를 할 때 모든 가중치 $w$의 분산을 $1/n$로 만들어야 한다는 점이다. <br />\n",
    "그리고 확률 변수 $X$와 스칼라 $a$에 대해서 $Var(aX)={ a }^{ 2 }Var(X)$이 성립하므로 분산이 $1/n$이 되기 위해서는 <br />표준 정규분포에서 값을 뽑아서 $a=\\sqrt { 1/n }$에 의한 scale을 해야한다. <br />\n",
    "그러므로 w = np.random.randn(n)/sqrt(n)으로 가중치를 초기화하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이와 유사한 내용의 연구가 [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) 에 자세히 나와 있다. 이 논문에서는\n",
    "초기화 공식으로 $Var(w)=\\frac { 2 }{ ({ n }_{ in }+{ n }_{ out }) }$을 추천한다. 여기서 { n }_{ in },{ n }_{ out }은 the previous 레이어와 the next 레이어의 unit 수 이다. <br />\n",
    "이 분석은 backpropagated 그라디언트의 분석에 기초한다. 이 주제와 관련한 최근 논문은 [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv-web3.library.cornell.edu/abs/1502.01852)by He et al.이다. <br /> \n",
    "특히 ReLU 뉴런에 대하여 초기화를 유도한다. 뉴럴 네트워크에서 뉴런들의 분산은 $2.0/n$이 되어야 한다고 결론을 내리고 있다. <br />\n",
    "즉, 이는 w = np.random.randn(n)\\*sqrt(2.0/n)을 이용하여 초기화 하는 것을 의미하며 <br /> \n",
    "이는 특히 ReLU 뉴런들이 사용되는 뉴럴 네트워크에 최근에 권장되고 있는 방식이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보정되지 않은 분산 문제를 해결하기 위한 또 다른 방법은 모든 가중치 행렬을 0으로 초기화 하는 것이다. <br />\n",
    "이때 대칭성을 깨기 위해서 모든 뉴런을 고정된 숫자의 아래 단계의 뉴런들과 무작위로 연결한다. <br /> \n",
    "(위의 작은 가우시안 분포로 부터 sampled된 가중치들과 함께) 보통 연결하는 뉴런의 수는 대략 10개 정도이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대칭성 깨기는 가중치들을 매우 작은 무작위한 값을 설정함으로서 해결이 되기 때문에 주로 bias는 0으로 초기화한다. <br />\n",
    "ReLU 연산의 비선형성 때문에 어떤 경우에는 0.01과 같은 작은 상수값을 사용하기도 하는데, <br /> \n",
    "이는 ReLU unit이 초기부터 fire되고 따라서 어떤 그라디언트 값이 뉴럴 네트워크에 propagate 하는 것을 보장할 수 있기 때문이다. <br />\n",
    "하지만 상수값을 사용하는 방식이 성능 향상을 언제나 보장하는 것인가에 대해서는 이견이 존재한다. <br /> \n",
    "(실제 몇몇 사례에서는 더 나쁜 결과를 볼 수 있다) <br />\n",
    "따라서 bias값을 0으로 초기화 하는 것이 더 일반적이라 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실전 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU 유닛을 사용하는 것을 추천하고, w = np.random.randn(n)\\*sqrt(2.0/n)초기화를 하는것이 요즘의 추세이다. <br />\n",
    "as discussed in [He et al.](https://arxiv-web3.library.cornell.edu/abs/1502.01852)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loffe 와 Szegedy에 의해 최근에 제안된 [Batch Normalization](https://arxiv.org/abs/1502.03167) 기법은 학습 초기 단계에서 activations가 뉴럴 네트워크 전체에 걸쳐 표준 정규분포를 갖도록 명백히 강제하는 기법으로 <br /> \n",
    "뉴럴 네트워크를 적절히 초기화함으로서 그동안의 골치거리들을 상당 부분을 해소해 주었다. <br />\n",
    "여기서 사용한 정규화(normalization) 기법이 단순 미분 가능한 연산이었기에 적용 가능하다. <br />\n",
    "실제 구현에서 BatchNorm 레이어를 fully-connected 레이어(혹은 Conv 레이어) 직후, <br /> \n",
    "비선형 연산 이전에 위치시키는 방법으로 이 기법을 뉴럴 네트워크에 적용할 수 있다. <br /> \n",
    "이 기법은 이미 뉴럴 네트워크 학습에서 일반적으로 사용되는 기법중 하나이다. <br />\n",
    "실제 적용 사례를 보면 Batch Normalization를 사용하여 학습한 뉴럴 네트워크는 특히 나쁜 초기화에도 강하다는 것이 밝혀졌다. <br />\n",
    "또한, Batch Normalization은 뉴럴 네트워크 내의 모든 레이어에서 수행하는 전처리로 해석되지만, <br /> \n",
    "미분 가능하다는 성질에 의해서 깔끔하게 네트워크 안으로 통합된다고 볼 수 있다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번 파트에서는 뉴럴 네트워크 학습에서 과적합을 막을 수 있는 몇 가지 방법을 소개한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 정규화(regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 일반적으로 사용되는 정규화 기법이다. 모든 파라미터들의 제곱 만큼의 크기를 목적 함수(objective)에 제약을 거는 방식으로 구현된다. <br />\n",
    "즉, 네트워크에서 모든 가중치 $w$에 대하여, 목적 함수(objective)에 $\\frac { 1 }{ 2 } \\lambda { w }^{ 2 }$를 더한다. (여기서 $\\lambda$는 정규화(regularization)의 강도) <br />\n",
    "보통 앞에 부분에 $\\frac { 1 }{ 2 } $ 있는데, 이렇게 함으로서 파라미터 $w$에 대한 이 term의 그라디언트가 2$\\lambda w$가 아닌 $\\lambda w$가 되기 때문이다. <br />\n",
    "L2 정규화(regularization)는 직관적으로 해석해보면 peaky 가중치 벡터들에 큰 페널티를 주어서, 가중치 벡터들이 가능한 defuse 되도록 한다. <br />\n",
    "선형 분류 장에서도 말했듯이, 이 과정은 가중치들과 입력 데이터들이 서로 곱해지는 연산이므로, <br /> \n",
    "네트워크가 특정 몇몇 입력 데이터를 많이 사용하기 보다는 모든 입력 데이터가 다소 작더라도 골고루 사용되도록 한다. <br />\n",
    "마지막으로, 디센트 그라디언트 업데이트를 하는 동안 L2 정규화(regularization)를 사용함으로서 <br />\n",
    "궁극적으로 모든 가중치가 선형적으로 감소하게 된다. W += -lambda\\*W이 0을 향한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 정규화(regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또다른 상대적으로 많이 사용되는 정규화(regularization) 기법으로 각각의 가중치 $w$에 대하여, 목적 함수에 $\\lambda |w|$ 항을 더한다. <br />\n",
    "다음 같이 L1 정규화(regularization)와 L2 정규화(regularization)를 동시에 사용할 수도 있다. <br />\n",
    "${\\lambda}_{1} |w|$+${\\lambda}_{2} |{w}^{2}|$. <br /> (this is called [Elastic net regularization](http://web.stanford.edu/~hastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&%20Hastie.pdf)) <br />\n",
    "L1 정규화(regularization)는 최적화 과정 동안 가중치 벡터들을 희소하게(거의 0에 가깝게)만드는 흥미로운 특성이 있다. <br />\n",
    "다시 말해, L1 정규화(regularization) 적용된 뉴런들은 결국 그들의 가장 중요한 입력들의 sparse한 부분 집합만을 사용하고, <br /> \n",
    "\"noisy\"한 입력 데이터에 거의 영향을 받지 않는다. <br />\n",
    "이에 반해, L2 정규화(regularization)를 적용한 최종 가중치 벡터들은 보통 작은 값들이 퍼져있는 형태로 나타난다. <br />\n",
    "실제로, 만약 명백한 feature selection이 고려된 것이 아니라면 많은 경우에 L2 정규화(regularization)를 사용하여 <br /> 훨씬 좋은 성능을 기대할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max norm constrains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다른 정규화(regularization) 기법의 하나로, 모든 뉴런에 대하여 가중치 벡터의 크기가 절대적 상한 선을 넘지 못하도록 enforce 하고 <br /> \n",
    "제약 조건을 enforce하기 위해 projected 그라디언트 디센트의 사용한다. <br /> \n",
    "실제로, 이것은 정상적으로 파라미터 업데이트를 수행하는 것에 대응하고, <br /> \n",
    "그러고나서 모든 뉴런의 가중치 벡터 $2\\overset { \\rightarrow  }{ w } $에 대해서 ${ \\parallel \\overset { \\rightarrow  }{ w } \\parallel  }_{ 2 }<2$를 만족하도록 제한을 건다. <br />\n",
    "일반적으로 $c$값은 on order of 3 or 4이다. 이 정규화(regularization) 기법을 적용한 몇몇 연구를 통하여 성능 향상이 있음이 알려졌다. <br />\n",
    "이 기법의 흥미로운 사실 중 하나는 학습률을 큰 값으로 설정하고 학습 시키더라도 뉴럴 네트워크가 폭발하지 않는 다는 것인데 <br /> \n",
    "이는 업데이트가 항상 제한된 범위 내에 있기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매우 효과적이고 간단하고 최근에 소개된 정규화(regularization) 방법으로 <br />\n",
    "[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)(pdf)에 소개되어있다.(by Srivastava et al.) <br /> \n",
    "위에서 소개한 다른 정규화(regularization) 기법(L1, L2, maxnorm)들과 상호 보환적인 방법으로 알려져 있다. <br />\n",
    "학습하는 동안 drop out은 뉴런의 active을 $p$ 확률(a hyperparameter)로 유지하고 다른 것들은 0으로 설정한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이 아이디어를 보여주는 [Dropout paper](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) 로부터 선택한 그림. <br /> \n",
    "Drop out은 학습하는 동안 전체 뉴럴 네트워크 내에서 뉴럴 네트워크를 sampling 하고, <br /> \n",
    "입력 자료에 기반해서 sampled된 네트워크의 파라미터들만 업데이트하는 것으로 해석할 수 있다. <br /> \n",
    "(하지만, 지수적 수의 sampled 네트워크들은 그들의 파라미터를 공유하기 때문에 서로 독립적이지 않다) <br /> \n",
    "테스트하는 동안에는 drop out이 적용되지 않고, 지수적 크기의 모든 sub-네트워크들의 앙상블을 따라 평균 예측을 평가하는 것으로 해석한다. <br />\n",
    "(앙상블에 대한 자세한 내용은 다음 장에)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-레이어 뉴럴 네트워크에 적용된 vanilla dropout 예제를 아래에 구현하였다. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Vanilla Dropout: Not recommended implementation (see notes below) \"\"\"\n",
    "\n",
    "p = 0.5 # probability of keeping a unit active. higher = less dropout\n",
    "\n",
    "def train_step(X):\n",
    "  \"\"\" X contains the data \"\"\"\n",
    "  \n",
    "  # forward pass for example 3-layer neural network\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "  U1 = np.random.rand(*H1.shape) < p # first dropout mask\n",
    "  H1 *= U1 # drop!\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  U2 = np.random.rand(*H2.shape) < p # second dropout mask\n",
    "  H2 *= U2 # drop!\n",
    "  out = np.dot(W3, H2) + b3\n",
    "  \n",
    "  # backward pass: compute gradients... (not shown)\n",
    "  # perform parameter update... (not shown)\n",
    "  \n",
    "def predict(X):\n",
    "  # ensembled forward pass\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations\n",
    "  out = np.dot(W3, H2) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드에서, train_step 함수의 안에 첫 번째 히든 레이어와 두 번째 히든 레이어 총 2 부분에서 drop out이 적용된 것을 볼 수 있다. <br />\n",
    "또한 입력 데이터 X를 위한 binary mask를 만들어 입력 레이어에서도 drop out을 적용할 수 있다. <br /> \n",
    "backward pass는 변하지 않으나, 당연히 generated masks U1, U2를 고려해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict 함수에는 더 이상 drop out을 적용하지 않았지만 두 히든 레이어의 출력들에 대하여 $p$만큼 scaling 하였다. <br />\n",
    "이것이 중요한 이유는 테스트를 시행할 때 모든 뉴런은 모든 그들의 입력들을 보기 때문이다. <br /> \n",
    "따라서 테스트 할 때 뉴런들의 출력들이 학습할 때 그들의 기대 출력들과 동일하도록 해야한다. <br />\n",
    "예를 들어, drop out 확률이 $p=0.5$인 경우를 가정해 보자. <br /> \n",
    "Test time에서 뉴런들의 출력을 반으로 줄여야한다. Training time에서 그들의 출력과 같은 출력을 가져야하기 때문이다. (기대값에 있어서) <br />\n",
    "이것을 보기위해, 뉴런 $x$의 출력(before drop out)에 대해 생각해보자. <br />\n",
    "drop out을 적용하였을때, 이 뉴런에서의 기대값은 $px+(1-p)0$가 된다. 왜냐하면 뉴런의 출력은 $1-p$의 확률로 0이 되기 때문이다. <br />\n",
    "Test time에서, 뉴런들이 항상 active를 유지할때, 동일한 기대값을 유지하기 위해서는 $x->px$로 보정해 주어야 한다. <br />\n",
    "Test time에서 이 감쇠(attenuation)을 수행하는 것은 모든 가능한 binary mask에 대하여 반복 프로세스와 <br />\n",
    "그들의 앙상블 예측을 수행하는것과 연관될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 소개한 방법의 바람직하지 않은 특성은 test time에서 activation에 $p$를 scale해야 한다는 것이다. <br />\n",
    "test-time 성능은 매우 중요한 이슈이기 때문에 train time에서 inverted dropout 방식이 항상 더 선호된다. <br />\n",
    "이는 train time에서 scaling 연산을 적용하고 test time 에서는 untouched된 상태로 forward pass를 남겨두는 방식이다. <br />\n",
    "이 기법의 또 다른 매력적인 특징은 만약 drop out을 적용하기로 한 곳에 수정을 하기로 했을 때 <br /> \n",
    "예측 코드를 untouched한 상태로 유지할 수 있다. <br />\n",
    "Inverted drop out은 다음과 같이 구현할 수 있다. <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Inverted Dropout: Recommended implementation example.\n",
    "We drop and scale at train time and don't do anything at test time.\n",
    "\"\"\"\n",
    "\n",
    "p = 0.5 # probability of keeping a unit active. higher = less dropout\n",
    "\n",
    "def train_step(X):\n",
    "  # forward pass for example 3-layer neural network\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "  U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!\n",
    "  H1 *= U1 # drop!\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!\n",
    "  H2 *= U2 # drop!\n",
    "  out = np.dot(W3, H2) + b3\n",
    "  \n",
    "  # backward pass: compute gradients... (not shown)\n",
    "  # perform parameter update... (not shown)\n",
    "  \n",
    "def predict(X):\n",
    "  # ensembled forward pass\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  out = np.dot(W3, H2) + b3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop out이 처음 소개된 이후로 실제 적용에서 나타난 성능 향상의 원인에 대한 이해와 <br />\n",
    "기존의 다른 정규화(regularization) 기법과의 관계등에 대한 <수많은 연구가 진행되었다. <br />\n",
    "관심있는 독자들을 위한 추천할만한 further reading은 다음과 같다.\n",
    "- [Dropout paper](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) by Srivastava et al. 2014.\n",
    "- [Dropout Training as Adaptive Regularization:](http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf) “we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass에서 노이즈 관련하여"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "넓은 의미에서 보자면 drop out은 뉴럴 네트워크의 forward pass에서 확률적 접근을 도입하는 것이라고 볼 수 있다. <br />\n",
    "testing하는 동안, 노이즈가 analytically(확률 $p$만큼 곱해진 drop out), numerically <br />\n",
    "(sampling, 서로 다른 무작위로 선택된 몇개의 forward pass를 수행한 결과의 평균) 감소한다. <br />\n",
    "[DropConnect](http://cs.nyu.edu/~wanli/dropc/)를 포함하여, 이런 관점에서의 다른 연구의 예제에서는 <br />\n",
    "forward pass를 하는 동안 가중치들의 집합의 값을 무작위로 하는 대신 0으로 설정한다. <br />\n",
    "ConvNet에서 이 주제는 stochastic pooling, fractional pooling, data augmentation등의 기법과 함께 이득을 기대할 수 있다. 이 방법들에 대한 자세한 사항은 나중에 알아볼 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias 정규화(regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형 분류 파트에서 설명했듯이, bias 파라미터들은 정규화(regularization)를 적용하지 않는 것이 일반적인데, <br /> \n",
    "곱셈을 통하여 데이터와 상호작용하지 않기 때문이다.<br />\n",
    "그러므로 최종 목적 함수(objective)에서 데이터 차원을 결정하는 요소로 작용하지 않는다. <br /> \n",
    "실제 적용 사례들을(적절한 데이터 전처리가 이루어진) 보면 bias에 정규화(regularization)를 적용하였을 때 <br />\n",
    "심각한 성능 저하가 나타나는 경우는 드문 것으로 알려져 있다. 이는 모든 가중치들과 비교했을 때 bias 텀의 갯수는 매우 작기 때문이다. <br />\n",
    "따라서 만약 더 나은 데이터 loss를 얻고 싶다면 분류기에 정규화(regularization)된 bias를 사용하여도 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 레이어마다 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막 출력 레이어를 제외하고 레이어를 각각 따로 정규화(regularization) 하는 것은 일반적인 방법은 아니다. <br />\n",
    "이 아이디어에 대해서는 적용한 논문수도 상대적으로 적다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 실전 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하나의 공통된 L2정규화를 사용하는 것은 일반적이다. 또한 모든 레이어 이후에 드랍 아웃을 적용하는 것 또한 일반적으로 많이 사용된다. <br />\n",
    "드랍 아웃 Rate로 p=0.5가 주로 사용되지만 검증과정에서 값을 조정하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 손실 함수들"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델의 복잡성에 페널티를 주는 방법으로 목적 함수(objective)의 정규화(regularization) 손실 부분에 대해 살펴보았다. <br />\n",
    "목적 함수(objective)의 두 번째 파트는, 지도 학습 문제에서 예측(e.g: 분류기의 클래스 스코어)과 <br />\n",
    "실제 ground truth 라벨 사이의 compatibility를 측정하는 데이터 손실에 대해서 살펴보는 것이다. <br />\n",
    "자료 손실은 모든 개별 예제의 데이터 손실의 평균의 형태를 취한다. 즉, $L = \\frac{1}{N} \\sum_i L_i$에서 $N$은 학습 자료의 숫자이다. <br /> \n",
    "뉴럴 네트워크의 출력 레이어의 activation $f = f(x_i; W)$를 경감시켜보자. <br />\n",
    "실전에서 해결하길 원하는 문제들에 대해 적용할 수 있는 형태들이 몇가지 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제들의 데이서 셋과 각 예제에 대하여 하나의 올바른 라벨(고정된 셋으로부터)을 가지고 있다고 가정한다. <br />\n",
    "가장 흔하게 사용되는 cost funtion들 중 하나는 SVM이다.(e.g. the Weston Watkins formulation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 사람들은 squared hinge loss(i.e: $\\max(0, f_j - f_{y_i} + 1)^2$을 대신 사용하는 것)보다 SVM이 더 낫다고 평가한다. <br />\n",
    "두 번째 일반적인 선택은 크로스 엔트로피 손실을 사용하는 소프트맥스 분류기이다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문제: 클래스의 수가 많을 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라벨의 집합이 매우 클 때(e.g. words in English dictionary or ImageNet which contains 22,00 categories), <br /> \n",
    "계층적 소프트맥스를 사용하는 것이 효과적일 수 있다. 계층적 소프트맥스는 라벨들을 트리로 분해한다. <br />\n",
    "각각의 라벨은 트리를 따라 길로서 표현되고, 소프트맥스 분류기는 왼쪽 가지와 오른쪽 가지 사이를 구분하기 위해 모든 노드를 트레이닝 시킨다. <br />\n",
    "트리의 구조는 성능에 매우 강한 영향력을 갖고 일반적으로 문제 의존적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 속성 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 손실들은 하나의 올바른 정답 $y_i$를 가정하였다. 그러나 만약 $y_i$가 모든 예제가 특정 속성을 가지거나 가지고 있지 않거나, <br /> \n",
    "그 특성들이 서로 배타적이지 않은 이항 벡터라면 어떻게 해야할까? <br />\n",
    "예를 들어, 인스타그램에서 특정 집합의 라벨은 모든 해시태그의 큰 집합에서 오고, 이미지는 여러 개의 라벨을 갖는다. <br />\n",
    "이 케이스에 대해 현명한 접근방법은 모든 단독 속성에 대해 독립적으로 이항 분류기를 만드는 것이다. <br />\n",
    "독립적인 카테고리에 대한 이항 분류기의 형식은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/8.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 식에서는, 합은 모든 카테고리 j에 대해 더해지고, <br />\n",
    "$y_ij$는 i 번째 예제가 j 번째 속성에 라벨링이 되어있는지에 의존하여 +1 또는 -1 이 선택되는 값이다. <br />\n",
    "그리고 스코어 벡터 $f_j$는 클래스가 현재를 예측하면 양수 그렇지 않으면 음수이다. <br />\n",
    "손실은 양의 예제가 +1 보다 작은 점수를 가지거나 음의 예제가 -1보다 큰 점수를 가질 때 누적된다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "이 손실에 대안으로서 모든 속성에 대해 독립적으로 로지스틱 회귀 분류기를 학습시킬수도 있다. <br />\n",
    "이항 로지스틱 회귀 분류기는 오직 두 개(0, 1)의 클래스를 가지고, 클래스 1의 확률을 다음과 같이 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/9.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "클래스가 1일 확률과 0일 확률을 더하면 1이기 때문에, 클래스가 0일 확률은 $P(y = 0 \\mid x; w, b) = 1 - P(y = 1 \\mid x; w,b)$이다. <br />\n",
    "따라서 $\\sigma (w^Tx + b) > 0.5$d이거나 스코어가 $w^Tx +b > 0$이면 양의 예제로 분류된다.(y=1) <br />\n",
    "그런 다음 손실 함수는 이 확률의 로그 우도를 극대화 한다. <br />\n",
    "간단히 다음과 같이 쓸 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/10.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "라벨 $y_{ij}$는 1(양) 또는 0(음)으로 가정하며, $\\sigma(\\cdot)$는 시그모이드 함수이다. <br />\n",
    "식 자체는 거창해보일지 모르나 $f$의 그라디언트 $\\partial{L_i} / \\partial{f_j} = y_{ij} - \\sigma(f_j)$는 매우 단순하고 직관적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "집의 가격이나 이미지에서 어떤 길이 같은 real-valued quantity를 예측하는 작업이다. <br />\n",
    "이 작업에서는, predicted quantity와 진짜 답 사이의 손실을 계산하는 것이 일반적이고, <br /> \n",
    "그런 다음 보통 차이의 L2 squared norm이나 L1 Norm의 차이를 측정한다. <br />\n",
    "L2 Norm squared은 단일 예제에서 다음과 같이 계산한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/11.png\" width=180 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목적 함수(objective)에서 L2 Norm이 제곱된 이유는 제곱이 단조 연산이기 때문에 최적 파라미터들의 변화없이 <br /> \n",
    "그라디언트가 더 간단해지기 때문이다. <br />\n",
    "L1 Norm은 각 차원을 따라 절대 값을 더하는 것으로 나타낼 수 있다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/12.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 하나의 quantity 이상 예측된다면, 합 $\\sum_j$는 바람직하게 예측된 모든 차원에 대하여 더해진다. <br />\n",
    "i번째 예제에서 j번째 차원만을 관찰하고 실제와 예측값이 차이를 $\\delta_{ij}$로 놓으면, <br />\n",
    "이 차원((i.e. $\\partial{L_i} / \\partial{f_j}$))에 대한 그라디언트는 L2 Norm의 $\\delta_{ij}$ 혹은 $sign(\\delta_{ij})$ 둘중 하나로 쉽게 유도된다. <br />\n",
    "즉, 스코어에 대한 그라디언트는 오차의 차이에 직집적으로 비례하거나, 고정되어서 차이의 sign를 물려받을 받을 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2의 손실은 소프트맥스처럼 더 안정적으로 손실을 최적화하기 어렵다. <br />\n",
    "직관적으로, 네트워크가 각각의 입력(이것의 증가율)에 대해 하나의 정확한 값을 출력해야하는 특정한 속성을 가져야하는 경우 매우 취약하다. <br /> \n",
    "각각의 스코어의 정확한 값이이 덜 중요한 소프트맥스는 이런 경우가 아니다. <br />\n",
    "그들의 magnitude가 적절한지가 문제이다. 게다가 L2 손실은 outlier들이 매우 큰 그라디언트를 유발할 수 있는 것에 취약하다. <br />\n",
    "회귀 문제를 만났을 때, 첫 번째로 고려해야할 것은 이것의 출력이 이항으로 나오는지에 대해 절대적으로 불충분한지 quantize하는 것이다. <br />\n",
    "예를 들어, 만약 상품에 대한 별점을 예측한다고 할 때, 회귀 손실보다 1-5개의 별을 매길 수 있는 5 indepdent 분류기를 쓰는 것이 훨씬 낫다. <br />\n",
    "분류기는 단지 하나의 출력에 대해 확신이 없는 indication이 아닌, 모든 회귀 출력들에 대해 분포를 제공한다. <br />\n",
    "만약 분류기가 적절하지 않다고 확신한다면 L2를 쓰되 조심해라. <br />\n",
    "예를 들어, L2 는 더 취약하다 그리고 네트워크(특히 L2 손실 바로 전 레이어에)에서 drop out에 적용하는 것은 좋은 생각이 아니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 회귀 task에 직면했을 때 첫 번째로 고려할 것은 이것이 절대적으로 필요한 것인가인지 생각하는 것이다. <br />\n",
    "대신, 출력들을 이항으로 분류하는 것이 그들에 대해서 언제나 가능할 때 분류기에 대하여 보다 더 큰 선호를 가져라."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structured prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured loss는 라벨들이 그래프나 트리 또는 다른 복잡한 객체같은 것들로 임의로 구조화 될 수 있는 case를 말한다. <br />\n",
    "보통 structures의 공간은 매우 크지만 쉽게 enumerable 할 수 없다고 가정한다. <br />\n",
    "structured SVM 손실 뒤의 기본적인 생각은 정확한 structure $y_i$ 그리고 부정확한 structure의 가장 높은 점수 사이의 마진을 요구하는 것이다. <br />\n",
    "이 문제는 그라디언트 디센트를 사용하여 간단한 제약없는 최적화를 푸는것처럼 일반적이지 않다. <br />\n",
    "대신에, 특별한 solvers들이 structure 공간에서 이득을 취할수 있다는 단순화된 특정한 가정들이 고안됨으로써 이득을 취할 수 있다. <br />\n",
    "이는 수업의 범위를 넘어선다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary:\n",
    "\n",
    "- The recommended preprocessing is to center the data to have mean of zero, and normalize its scale to [-1, 1] along each feature\n",
    "- Initialize the weights by drawing them from a gaussian distribution with standard deviation of $\\sqrt { \\frac { 2 }{ n }  } $, where $n$ is the number of inputs to the neuron. E.g. in numpy: w = np.random.randn(n) * sqrt(2.0/n).\n",
    "- Use L2 regularization and dropout (the inverted version)\n",
    "- Use batch normalization\n",
    "- We discussed different tasks you might want to perform in practice, and the most common loss functions for each task\n",
    "\n",
    "We’ve now preprocessed the data and set up and initialized the model. In the next section we will look at the learning process and its dynamics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
