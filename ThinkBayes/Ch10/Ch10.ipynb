{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 근사 베이지안 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) 변이 가설"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 장에서 다룰 것은 변이 가설이다. 이는 다음과 같다. <br />\n",
    "\"남자가 여자보다 능력 범위가 넓고, 특히 지적인 면에서 훨씬 뛰어나다고 주장한 조안 미켈이 19세기 초에 만든 분야이다. <br/>\n",
    "그는 대부분의 천재와 대부분의 정신적 미숙아는 남자라고 믿었다. 그는 남자는 '우수한 동물'이라고 생각해서 <br />\n",
    "여성의 변이 부족은 열등함의 표시라고 결론지었다.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 만약 여성이 실제로 남자보다 가변적인 것이 드러나면 미켈 역시 이를 열등함의 표시라고 받아들여야 할 것이다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미국의 남녀 성인이 키를 직접 입력한 데이터를 살펴보자. <br />\n",
    "데이터셋에는 15,507명의 남성과 254,722명의 여성의 응답 내용이 들어있다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 데이터의 내용은 다음과 같다.\n",
    "- 남성의 평균 키는 178cm이고, 여성의 평균 키는 163cm이다. 즉 평균적으로 남성이 더 크다.\n",
    "- 남성의 표준편차는 7.7cm이고, 여성의 경우는 7.3cm이다. 따라서 남성의 키가 더 가변적이다.\n",
    "- 하지만 그룹 간의 변이를 비교할때 표준편차를 평균으로 나누는 변동 계수(Coefficient of variation, CV)를 사용하면 <br />\n",
    "남성의 경우 CV가 0.0433이고, 여성의 경우 0.0444이다. 이 값은 변이가 원 수치와 관계없는 무차원적 측정치이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자들이 매우 비슷하므로 데이터셋이 변이 가설을 증명하기에는 빈약하다고 결론을 내릴 수 있다. <br />\n",
    "하지만 베이지안 메서드를 사용하면 결론을 더 명확하게 만들 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전체 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male 154407\n",
      "smallest [  61.   74.   76.   81.   86.   89.   89.   91.   97.  101.]\n",
      "largest [ 218.  221.  221.  221.  221.  225.  226.  229.  229.  236.]\n",
      "173.0 178.0 183.0\n",
      "classical estimators 178.37680558 7.43067384489\n",
      "median, s 178.37680558 7.43067384489\n",
      "MLE (178.37680557998382, 7.4306738448888865)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choi/anaconda/lib/python3.6/site-packages/matplotlib/axes/_axes.py:545: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing variability_posterior_male.pdf\n",
      "Writing variability_posterior_male.eps\n",
      "marginal mu 178.37680558 0.000350420307418\n",
      "marginal sigma 7.43071586209 0.000175210671709\n",
      "female 254722\n",
      "smallest [ 61.  61.  64.  66.  74.  81.  89.  89.  89.  91.]\n",
      "largest [ 213.  213.  213.  213.  221.  226.  229.  229.  229.  229.]\n",
      "157.0 163.0 168.0\n",
      "classical estimators 163.346191385 7.15672683982\n",
      "median, s 163.346191385 7.15672683982\n",
      "MLE (163.34619138541342, 7.156726839817523)\n",
      "Writing variability_posterior_female.pdf\n",
      "Writing variability_posterior_female.eps\n",
      "marginal mu 163.346191385 0.000197042736953\n",
      "marginal sigma 7.15675137089 9.85215450402e-05\n",
      "CV posterior mean 0.04165741123\n",
      "CV posterior mean 0.0438133963397\n",
      "Writing variability_cv.pdf\n",
      "Writing variability_cv.eps\n",
      "female bigger 1.0000000000000033\n",
      "male bigger 0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy\n",
    "import pickle\n",
    "import numpy\n",
    "import random\n",
    "import scipy\n",
    "\n",
    "import brfss\n",
    "\n",
    "import thinkplot\n",
    "import thinkbayes2\n",
    "\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "\n",
    "NUM_SIGMAS = 1\n",
    "\n",
    "class Height(thinkbayes2.Suite, thinkbayes2.Joint):\n",
    "    \"\"\"Hypotheses about parameters of the distribution of height.\"\"\"\n",
    "\n",
    "    def __init__(self, mus, sigmas, label=None):\n",
    "        \"\"\"Makes a prior distribution for mu and sigma based on a sample.\n",
    "\n",
    "        mus: sequence of possible mus\n",
    "        sigmas: sequence of possible sigmas\n",
    "        label: string label for the Suite\n",
    "        \"\"\"\n",
    "        pairs = [(mu, sigma) \n",
    "                 for mu in mus\n",
    "                 for sigma in sigmas]\n",
    "\n",
    "        thinkbayes2.Suite.__init__(self, pairs, label=label)\n",
    "\n",
    "    def Likelihood(self, data, hypo):\n",
    "        \"\"\"Computes the likelihood of the data under the hypothesis.\n",
    "\n",
    "        Args:\n",
    "            hypo: tuple of hypothetical mu and sigma\n",
    "            data: float sample\n",
    "\n",
    "        Returns:\n",
    "            likelihood of the sample given mu and sigma\n",
    "        \"\"\"\n",
    "        x = data\n",
    "        mu, sigma = hypo\n",
    "        like = scipy.stats.norm.pdf(x, mu, sigma)\n",
    "        return like\n",
    "\n",
    "    def LogLikelihood(self, data, hypo):\n",
    "        \"\"\"Computes the log likelihood of the data under the hypothesis.\n",
    "\n",
    "        Args:\n",
    "            data: a list of values\n",
    "            hypo: tuple of hypothetical mu and sigma\n",
    "\n",
    "        Returns:\n",
    "            log likelihood of the sample given mu and sigma (unnormalized)\n",
    "        \"\"\"\n",
    "        x = data\n",
    "        mu, sigma = hypo\n",
    "        loglike = EvalNormalLogPdf(x, mu, sigma)\n",
    "        return loglike\n",
    "\n",
    "    def LogUpdateSetFast(self, data):\n",
    "        \"\"\"Updates the suite using a faster implementation.\n",
    "\n",
    "        Computes the sum of the log likelihoods directly.\n",
    "\n",
    "        Args:\n",
    "            data: sequence of values\n",
    "        \"\"\"\n",
    "        xs = tuple(data)\n",
    "        n = len(xs)\n",
    "\n",
    "        for hypo in self.Values():\n",
    "            mu, sigma = hypo\n",
    "            total = Summation(xs, mu)\n",
    "            loglike = -n * math.log(sigma) - total / 2 / sigma**2\n",
    "            self.Incr(hypo, loglike)\n",
    "\n",
    "    def LogUpdateSetMeanVar(self, data):\n",
    "        \"\"\"Updates the suite using ABC and mean/var.\n",
    "\n",
    "        Args:\n",
    "            data: sequence of values\n",
    "        \"\"\"\n",
    "        xs = data\n",
    "        n = len(xs)\n",
    "\n",
    "        m = numpy.mean(xs)\n",
    "        s = numpy.std(xs)\n",
    "\n",
    "        self.LogUpdateSetABC(n, m, s)\n",
    "\n",
    "    def LogUpdateSetMedianIPR(self, data):\n",
    "        \"\"\"Updates the suite using ABC and median/iqr.\n",
    "\n",
    "        Args:\n",
    "            data: sequence of values\n",
    "        \"\"\"\n",
    "        xs = data\n",
    "        n = len(xs)\n",
    "\n",
    "        # compute summary stats\n",
    "        median, s = MedianS(xs, num_sigmas=NUM_SIGMAS)\n",
    "        print('median, s', median, s)\n",
    "\n",
    "        self.LogUpdateSetABC(n, median, s)\n",
    "\n",
    "    def LogUpdateSetABC(self, n, m, s):\n",
    "        \"\"\"Updates the suite using ABC.\n",
    "\n",
    "        n: sample size\n",
    "        m: estimated central tendency\n",
    "        s: estimated spread\n",
    "        \"\"\"\n",
    "        for hypo in sorted(self.Values()):\n",
    "            mu, sigma = hypo\n",
    "\n",
    "            # compute log likelihood of m, given hypo\n",
    "            stderr_m = sigma / math.sqrt(n)\n",
    "            loglike = EvalNormalLogPdf(m, mu, stderr_m)\n",
    "\n",
    "            #compute log likelihood of s, given hypo\n",
    "            stderr_s = sigma / math.sqrt(2 * (n-1))\n",
    "            loglike += EvalNormalLogPdf(s, sigma, stderr_s)\n",
    "\n",
    "            self.Incr(hypo, loglike)\n",
    "\n",
    "\n",
    "def EvalNormalLogPdf(x, mu, sigma):\n",
    "    \"\"\"Computes the log PDF of x given mu and sigma.\n",
    "\n",
    "    x: float values\n",
    "    mu, sigma: paramemters of Normal\n",
    "\n",
    "    returns: float log-likelihood\n",
    "    \"\"\"\n",
    "    return scipy.stats.norm.logpdf(x, mu, sigma)\n",
    "\n",
    "\n",
    "def FindPriorRanges(xs, num_points, num_stderrs=3.0, median_flag=False):\n",
    "    \"\"\"Find ranges for mu and sigma with non-negligible likelihood.\n",
    "\n",
    "    xs: sample\n",
    "    num_points: number of values in each dimension\n",
    "    num_stderrs: number of standard errors to include on either side\n",
    "    \n",
    "    Returns: sequence of mus, sequence of sigmas    \n",
    "    \"\"\"\n",
    "    def MakeRange(estimate, stderr):\n",
    "        \"\"\"Makes a linear range around the estimate.\n",
    "\n",
    "        estimate: central value\n",
    "        stderr: standard error of the estimate\n",
    "\n",
    "        returns: numpy array of float\n",
    "        \"\"\"\n",
    "        spread = stderr * num_stderrs\n",
    "        array = numpy.linspace(estimate-spread, estimate+spread, num_points)\n",
    "        return array\n",
    "\n",
    "    # estimate mean and stddev of xs\n",
    "    n = len(xs)\n",
    "    if median_flag:\n",
    "        m, s = MedianS(xs, num_sigmas=NUM_SIGMAS)\n",
    "    else:\n",
    "        m = numpy.mean(xs)\n",
    "        s = numpy.std(xs)\n",
    "\n",
    "    print('classical estimators', m, s)\n",
    "\n",
    "    # compute ranges for m and s\n",
    "    stderr_m = s / math.sqrt(n)\n",
    "    mus = MakeRange(m, stderr_m)\n",
    "\n",
    "    stderr_s = s / math.sqrt(2 * (n-1))\n",
    "    sigmas = MakeRange(s, stderr_s)\n",
    "\n",
    "    return mus, sigmas\n",
    "\n",
    "\n",
    "def Summation(xs, mu, cache={}):\n",
    "    \"\"\"Computes the sum of (x-mu)**2 for x in t.\n",
    "\n",
    "    Caches previous results.\n",
    "\n",
    "    xs: tuple of values\n",
    "    mu: hypothetical mean\n",
    "    cache: cache of previous results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return cache[xs, mu]\n",
    "    except KeyError:\n",
    "        ds = [(x-mu)**2 for x in xs]\n",
    "        total = sum(ds)\n",
    "        cache[xs, mu] = total\n",
    "        return total\n",
    "\n",
    "\n",
    "def CoefVariation(suite):\n",
    "    \"\"\"Computes the distribution of CV.\n",
    "\n",
    "    suite: Pmf that maps (x, y) to z\n",
    "\n",
    "    Returns: Pmf object for CV.\n",
    "    \"\"\"\n",
    "    pmf = thinkbayes2.Pmf()\n",
    "    for (m, s), p in suite.Items():\n",
    "        pmf.Incr(s/m, p)\n",
    "    return pmf\n",
    "\n",
    "\n",
    "def PlotCdfs(d, labels):\n",
    "    \"\"\"Plot CDFs for each sequence in a dictionary.\n",
    "\n",
    "    Jitters the data and subtracts away the mean.\n",
    "\n",
    "    d: map from key to sequence of values\n",
    "    labels: map from key to string label\n",
    "    \"\"\"\n",
    "    thinkplot.Clf()\n",
    "    for key, xs in d.items():\n",
    "        mu = thinkbayes2.Mean(xs)\n",
    "        xs = thinkbayes2.Jitter(xs, 1.3)\n",
    "        xs = [x-mu for x in xs]\n",
    "        cdf = thinkbayes2.MakeCdfFromList(xs)\n",
    "        thinkplot.Cdf(cdf, label=labels[key])\n",
    "    thinkplot.Show()\n",
    "                  \n",
    "\n",
    "def PlotPosterior(suite, pcolor=False, contour=True):\n",
    "    \"\"\"Makes a contour plot.\n",
    "    \n",
    "    suite: Suite that maps (mu, sigma) to probability\n",
    "    \"\"\"\n",
    "    thinkplot.Clf()\n",
    "    thinkplot.Contour(suite.GetDict(), pcolor=pcolor, contour=contour)\n",
    "\n",
    "    thinkplot.Save(root='variability_posterior_%s' % suite.label,\n",
    "                title='Posterior joint distribution',\n",
    "                xlabel='Mean height (cm)',\n",
    "                ylabel='Stddev (cm)')\n",
    "\n",
    "\n",
    "def PlotCoefVariation(suites):\n",
    "    \"\"\"Plot the posterior distributions for CV.\n",
    "\n",
    "    suites: map from label to Pmf of CVs.\n",
    "    \"\"\"\n",
    "    thinkplot.Clf()\n",
    "    thinkplot.PrePlot(num=2)\n",
    "\n",
    "    pmfs = {}\n",
    "    for label, suite in suites.items():\n",
    "        pmf = CoefVariation(suite)\n",
    "        print('CV posterior mean', pmf.Mean())\n",
    "        cdf = thinkbayes2.MakeCdfFromPmf(pmf, label)\n",
    "        thinkplot.Cdf(cdf)\n",
    "    \n",
    "        pmfs[label] = pmf\n",
    "\n",
    "    thinkplot.Save(root='variability_cv',\n",
    "                xlabel='Coefficient of variation',\n",
    "                ylabel='Probability')\n",
    "\n",
    "    print('female bigger', thinkbayes2.PmfProbGreater(pmfs['female'],\n",
    "                                                     pmfs['male']))\n",
    "    print('male bigger', thinkbayes2.PmfProbGreater(pmfs['male'],\n",
    "                                                   pmfs['female']))\n",
    "\n",
    "\n",
    "def PlotOutliers(samples):\n",
    "    \"\"\"Make CDFs showing the distribution of outliers.\"\"\"\n",
    "    cdfs = []\n",
    "    for label, sample in samples.items():\n",
    "        outliers = [x for x in sample if x < 150]\n",
    "\n",
    "        cdf = thinkbayes2.MakeCdfFromList(outliers, label)\n",
    "        cdfs.append(cdf)\n",
    "\n",
    "    thinkplot.Clf()\n",
    "    thinkplot.Cdfs(cdfs)\n",
    "    thinkplot.Save(root='variability_cdfs',\n",
    "                title='CDF of height',\n",
    "                xlabel='Reported height (cm)',\n",
    "                ylabel='CDF')\n",
    "\n",
    "\n",
    "def PlotMarginals(suite):\n",
    "    \"\"\"Plots marginal distributions from a joint distribution.\n",
    "\n",
    "    suite: joint distribution of mu and sigma.\n",
    "    \"\"\"\n",
    "    thinkplot.Clf()\n",
    "\n",
    "    pyplot.subplot(1, 2, 1)\n",
    "    pmf_m = suite.Marginal(0)\n",
    "    cdf_m = thinkbayes2.MakeCdfFromPmf(pmf_m)\n",
    "    thinkplot.Cdf(cdf_m)\n",
    "\n",
    "    pyplot.subplot(1, 2, 2)\n",
    "    pmf_s = suite.Marginal(1)\n",
    "    cdf_s = thinkbayes2.MakeCdfFromPmf(pmf_s)\n",
    "    thinkplot.Cdf(cdf_s)\n",
    "\n",
    "    thinkplot.Show()\n",
    "\n",
    "\n",
    "def ReadHeights(nrows=None):\n",
    "    \"\"\"Read the BRFSS dataset, extract the heights and pickle them.\n",
    "\n",
    "    nrows: number of rows to read\n",
    "    \"\"\"\n",
    "    resp = brfss.ReadBrfss(nrows=nrows).dropna(subset=['sex', 'htm3'])\n",
    "    groups = resp.groupby('sex')\n",
    "\n",
    "    d = {}\n",
    "    for name, group in groups:\n",
    "        d[name] = group.htm3.values\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "def UpdateSuite1(suite, xs):\n",
    "    \"\"\"Computes the posterior distibution of mu and sigma.\n",
    "\n",
    "    Computes untransformed likelihoods.\n",
    "\n",
    "    suite: Suite that maps from (mu, sigma) to prob\n",
    "    xs: sequence\n",
    "    \"\"\"\n",
    "    suite.UpdateSet(xs)\n",
    "\n",
    "\n",
    "def UpdateSuite2(suite, xs):\n",
    "    \"\"\"Computes the posterior distibution of mu and sigma.\n",
    "\n",
    "    Computes log likelihoods.\n",
    "\n",
    "    suite: Suite that maps from (mu, sigma) to prob\n",
    "    xs: sequence\n",
    "    \"\"\"\n",
    "    suite.Log()\n",
    "    suite.LogUpdateSet(xs)\n",
    "    suite.Exp()\n",
    "    suite.Normalize()\n",
    "\n",
    "\n",
    "def UpdateSuite3(suite, xs):\n",
    "    \"\"\"Computes the posterior distibution of mu and sigma.\n",
    "\n",
    "    Computes log likelihoods efficiently.\n",
    "\n",
    "    suite: Suite that maps from (mu, sigma) to prob\n",
    "    t: sequence\n",
    "    \"\"\"\n",
    "    suite.Log()\n",
    "    suite.LogUpdateSetFast(xs)\n",
    "    suite.Exp()\n",
    "    suite.Normalize()\n",
    "\n",
    "\n",
    "def UpdateSuite4(suite, xs):\n",
    "    \"\"\"Computes the posterior distibution of mu and sigma.\n",
    "\n",
    "    Computes log likelihoods efficiently.\n",
    "\n",
    "    suite: Suite that maps from (mu, sigma) to prob\n",
    "    t: sequence\n",
    "    \"\"\"\n",
    "    suite.Log()\n",
    "    suite.LogUpdateSetMeanVar(xs)\n",
    "    suite.Exp()\n",
    "    suite.Normalize()\n",
    "\n",
    "\n",
    "def UpdateSuite5(suite, xs):\n",
    "    \"\"\"Computes the posterior distibution of mu and sigma.\n",
    "\n",
    "    Computes log likelihoods efficiently.\n",
    "\n",
    "    suite: Suite that maps from (mu, sigma) to prob\n",
    "    t: sequence\n",
    "    \"\"\"\n",
    "    suite.Log()\n",
    "    suite.LogUpdateSetMedianIPR(xs)\n",
    "    suite.Exp()\n",
    "    suite.Normalize()\n",
    "\n",
    "\n",
    "def MedianIPR(xs, p):\n",
    "    \"\"\"Computes the median and interpercentile range.\n",
    "\n",
    "    xs: sequence of values\n",
    "    p: range (0-1), 0.5 yields the interquartile range\n",
    "\n",
    "    returns: tuple of float (median, IPR)\n",
    "    \"\"\"\n",
    "    cdf = thinkbayes2.MakeCdfFromList(xs)\n",
    "    median = cdf.Percentile(50)\n",
    "\n",
    "    alpha = (1-p) / 2\n",
    "    ipr = cdf.Value(1-alpha) - cdf.Value(alpha)\n",
    "    return median, ipr\n",
    "\n",
    "\n",
    "def MedianS(xs, num_sigmas):\n",
    "    \"\"\"Computes the median and an estimate of sigma.\n",
    "\n",
    "    Based on an interpercentile range (IPR).\n",
    "\n",
    "    factor: number of standard deviations spanned by the IPR\n",
    "    \"\"\"\n",
    "    half_p = thinkbayes2.StandardNormalCdf(num_sigmas) - 0.5\n",
    "    median, ipr = MedianIPR(xs, half_p * 2)\n",
    "    s = ipr / 2 / num_sigmas\n",
    "\n",
    "    return median, s\n",
    "\n",
    "def Summarize(xs):\n",
    "    \"\"\"Prints summary statistics from a sequence of values.\n",
    "\n",
    "    xs: sequence of values\n",
    "    \"\"\"\n",
    "    # print smallest and largest\n",
    "    xs.sort()\n",
    "    print('smallest', xs[:10])\n",
    "    print('largest', xs[-10:])\n",
    "\n",
    "    # print median and interquartile range\n",
    "    cdf = thinkbayes2.MakeCdfFromList(xs)\n",
    "    print(cdf.Percentile(25), cdf.Percentile(50), cdf.Percentile(75))\n",
    "\n",
    "\n",
    "def RunEstimate(update_func, num_points=31, median_flag=False):\n",
    "    \"\"\"Runs the whole analysis.\n",
    "\n",
    "    update_func: which of the update functions to use\n",
    "    num_points: number of points in the Suite (in each dimension)\n",
    "    \"\"\"\n",
    "    d = ReadHeights(nrows=None)\n",
    "    labels = {1:'male', 2:'female'}\n",
    "\n",
    "    # PlotCdfs(d, labels)\n",
    "\n",
    "    suites = {}\n",
    "    for key, xs in d.items():\n",
    "        label = labels[key]\n",
    "        print(label, len(xs))\n",
    "        Summarize(xs)\n",
    "\n",
    "        xs = thinkbayes2.Jitter(xs, 1.3)\n",
    "\n",
    "        mus, sigmas = FindPriorRanges(xs, num_points, median_flag=median_flag)\n",
    "        suite = Height(mus, sigmas, label)\n",
    "        suites[label] = suite\n",
    "        update_func(suite, xs)\n",
    "        print('MLE', suite.MaximumLikelihood())\n",
    "\n",
    "        PlotPosterior(suite)\n",
    "\n",
    "        pmf_m = suite.Marginal(0)\n",
    "        pmf_s = suite.Marginal(1)\n",
    "        print('marginal mu', pmf_m.Mean(), pmf_m.Var())\n",
    "        print('marginal sigma', pmf_s.Mean(), pmf_s.Var())\n",
    "\n",
    "        # PlotMarginals(suite)\n",
    "\n",
    "    PlotCoefVariation(suites)\n",
    "\n",
    "\n",
    "def main():\n",
    "    random.seed(17)\n",
    "\n",
    "    func = UpdateSuite5\n",
    "    median_flag = (func == UpdateSuite5)\n",
    "    RunEstimate(func, median_flag=median_flag)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### (2) 평균과 표준편차\n",
    "9장에서 두 변수를 결합 분포를 통해 동시에 추정하였다. <br />\n",
    "이 장에서는 가우시안 분포의 변수인 평균 mu와 표준편차 sigma를 동일한 방식으로 추정할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문제에서는 각 mu와 sigma의 쌍과 이 쌍이 나타날 확률의 연결 내용을 만드는 Heught Suite를 정의하겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Height(thinkbayes2.Suite, thinkbayes2.Joint):\n",
    "    def __init__(self, mus, sigmas):\n",
    "        thinkbayes2.Suite.__init__(self)\n",
    "        \n",
    "        pairs = [(mu, sigma) \n",
    "                 for mu in mus\n",
    "                 for sigma in sigmas]\n",
    "        \n",
    "        thinkbayes.Suite.__init__(self, pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mus는 mu의 가능한 값의 연속이고, sigmas는 sigma의 값의 연속이다. 모든 mu, sigma 쌍의 사전 분포는 균등 분포다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mu와 sigma의 가설 값이 주어졌을 때 특정 값 x에 대한 우도를 계산할 수 있다. <br /> \n",
    "이 기능은 scipy.stats.norm.pdf()가 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Height\n",
    "\n",
    "    def Likelihood(self, data, hypo):\n",
    "        x = data\n",
    "        mu, sigma = hype\n",
    "        like = scipy.stats.norm.pdf(x, mu, sigma)\n",
    "        return like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리의 목적에서 우리가 원하는 것은 확률의 비율이다. 이런 목적으로는 확률 밀도가 적절하다. <br />\n",
    "이 문제의 가장 어려운 부분은 mus와 sigmas의 적절한 범위를 고르는 일이다. <br />\n",
    "범위가 너무 작으면 몇가지 무시해서는 안 될 확률 값을 지나칠 수 있고, <br />\n",
    "범위가 너무 크면 답은 맞겠지만 쓸데없이 연산 능력을 낭비할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 베이지안 기술을 더 효율적으로 쓰기 위해 고전적 추정 방법을 사용한다. <br />\n",
    "mu와 sigma의 적절한 위치를 찾는데 고전적 추정치를 사용하고, 이 추정값의 적절한 분포를 선택하기 위해 표준편차를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "분포의 진짜 변수가 mu와 sigma이고, 여기서 n개의 샘플을 취할 때 mu의 추정치는 샘플의 평균인 m이다. <br />\n",
    "또한 sigma의 추정치는 샘플의 표준편차인 s다. 추정된 mu의 표준편차는 s/root(n)이고 <br />\n",
    "sigma의 추정치인 표준 오차는 s/root(2(n-1)) 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FindPriorRange(xs, num_points, num_stderrs=3.0):\n",
    "    \n",
    "    # m과 s를 계산함\n",
    "    \n",
    "    n = len(xs)\n",
    "    m = numpy.me(xs)\n",
    "    s = numpy.std(xs)\n",
    "    \n",
    "    # m과 s의 범위를 계산\n",
    "    \n",
    "    stderr_m = s / math.sqrt(n)\n",
    "    mus = MakeRange(m, stderr_m)\n",
    "    \n",
    "    stderr_s = s / marh.sqrt(2*(n-1))\n",
    "    sigmas = MakeRange(s, stderr_s)\n",
    "    \n",
    "    return mus, sigmas    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xs는 데이터셋이다. num_points는 범위 내의 필요한 값의 갯수다. num_stderrs는 표준 오차 수 내의 추정치의 양의 값 범위다.<br />\n",
    "결과 값은 연속값 mus와 sigmas의 쌍이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 코드는 MakeRange다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def MakeRange(estimate, stderr):\n",
    "        spread = stderr*num_stderrs\n",
    "        array = numpy.linspace(estimate - spread,\n",
    "                               estimate + spread,\n",
    "                               num_points)\n",
    "        return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num.linspace는 estimate-spread와 estimate+spread사이에 이 두 값을 포함하여 간격이 동일한 원소들의 행렬이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) 갱신"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 Suite를 만들고 갱신하는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    mus, sigmas = FindPriorRanges(xs, num_points)\n",
    "    suite = Height(mus, sigmas)\n",
    "    suite.UpdateSet(xs)\n",
    "    print (suite.MaximumLikelihood())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 사전 분포에서 데이터의 범위를 선택한 후, 갱신한 데이터를 다시 사용한다. <br />\n",
    "보통 같은 데이터를 두 번 사용하면 안된다. 하지만 이 경우에는 괜찮다. <br />\n",
    "이유는 수 많은 매우 작은 값에 대해서는 계산하지 않기 위해서이다.\n",
    "num_stderr = 4 일때 범위는 무시하면 안 되는 우도에 대한 모든 값을 포함할 정도로 충분히 크다. <br />\n",
    "따라서 좀 더 커진다고 해도 결과에는 아무런 영향을 끼치지 않는다. <br />\n",
    "실제로는, 사전 분포는 mu와 sigma의 모든 값에 대해 균등 분포이지만, 계산의 효율성을 위해 필요없는 값들은 무시한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) CV의 사후 분포"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CoefVariation(suite):\n",
    "    pmf = thinkbayes2.Pmf()\n",
    "    for (mu, sigma), p in suite.Items():\n",
    "        pmf.Incr(sigma/mu, p)\n",
    "    return pmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 후 thinkbayes2.PmfProbGreater를 사용해서 남자가 보다 변이성이 높은 확률을 계산할 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) 언더플로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "우도 계산에 확률 밀도를 사용하는데, 연속형 분포의 경우 밀도가 매우 작기 때문에 이를 서로 곱하면 결과는 매우 작아진다. <br />\n",
    "이를 언더플로(Under flow)라고 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 위한 해법으로 로그 변환하에서 우도를 계산하는 방법이 있다. <br />\n",
    "이 방법에서는 작은 값들을 곱하는 대신 로그 우도를 더한다. <br />\n",
    "Log는 Pmf내의 확률 로그를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Class Pmf\n",
    "    def Log(self):\n",
    "        m = self.MaxLike()\n",
    "        for x, p in self.d.iteritems():\n",
    "        if p:\n",
    "            self.Set(x, math.log(p/m))\n",
    "        else:\n",
    "            self.Remove(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "로그 변환 전에 Log는 MaxLike를 사용해서 Pmf에서 가장 높은 확률 m을 찾는다. <br />\n",
    "이 메서드에서는 모든 확률을 m으로 나누므로 가장 높은 확률이 log0인 1로 정규화된다. <br />\n",
    "다른 로그 확률은 모두 음수가 된다. 만약 0인 Pmf값이 있다면 이 값은 제거된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pmf가 로그 변환된 상태에서는 Update, UpdateSet, Normalize를 사용할 수 없다. <br />\n",
    "따라서 변형된 메서드인 LogUpdate와 LogUpdateSet을 사용해야 한다. <br />\n",
    "다음은 LogUpdateSet을 구현한 내용이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Suite\n",
    "    \n",
    "    def LogUpdateSet(self, dataset):\n",
    "        for data in dataset:\n",
    "            self.LogUpdate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LogUpdateSet은 데이터를 따라 반복하면서 LogUpdate를 호출한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Suite\n",
    "\n",
    "    def LogUpdate(self, data):\n",
    "        for hypo in self.Values():\n",
    "            like = self.LogLikelihood(data, hypo)\n",
    "            self.Incr(hypo, like)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LogUpdate는 Likelihood 대신 LogLikelihood를 호출하고, Mult 대신 Incr을 호출한다는 점을 제외하면 Update와 유사하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "로그 우도를 사용하면 언더플로에서 문제가 생기지 않지만, 로그 변환하에서는 Pmf로 더 이상 할 수 있는 것이 없다. <br />\n",
    "따라서 이미 변환된 것을 Exp를 사용해서 다시 원래대로 되돌린다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Pmf\n",
    "    \n",
    "    def Exp(self):\n",
    "        m = self.MaxLike()\n",
    "        for x, p in self.d.iteritems():\n",
    "            self.set(x, math.exp(p-m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 로그 우도가 매우 큰 음수라면, 결과 우도는 언더플로일 것이므로 Exp가 최대 로그 우도 m을 찾아서 <br />\n",
    "m을 찾아서 모든 우도를 m을 사용해 상향 이동시킨다. 결과 분포는 1에 대한 최대 우도를 갖는다. <br />\n",
    "이 프로세스는 로그 변환된 내용을 정확도 손실을 최소로 해서 원래 상태로 되돌린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6) 로그 우도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 LogLikelihood이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Height\n",
    "    \n",
    "    def LogLikelihood(self, data, hypo):\n",
    "        x = data\n",
    "        mu, sigma = hypo\n",
    "        loglike = scipy.stats.norm.logpdf(x, mu, sigma)\n",
    "        return loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "norm.logpdf는 가우시안 PDF의 로그 우도를 계산한다. 다음은 전체 갱신 프로세스의 내용이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정리해보면 Log는 로그 변환하에서 스윗을 실행한다. LogUpdateSet은 LogUpdate를 호출하고 이 함수는 LogLikelihood를 호출한다. <br />\n",
    "LogUpdate는 Pmf.Incr을 사용하여 로그 우도를 더함으로써 우도를 곱하는 것과 동일한 결과를 낸다. <br />\n",
    "갱신 후에는 로그 우도가 매우 큰 음수가 되므로 언더플로가 되는 것을 방지하기 위해 원상 복구하기 전에 Exp를 사용하여 상향 이동시킨다. <br />\n",
    "스윗이 원상 복구되면, 확률은 다시 선형으로 되어 Normalize를 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (7) 약간의 최적화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suite.LogUpdateSet은 각 데이터 지점에 대해 한 번씩 LogUpdate를 호출한다. <br /> \n",
    "이를 전체 데이터셋에 대해 우도를 한 번에 계산하면 속도를 계선할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가우시안 PDF에서 시작해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 (상수항에 대한) 로그를 계산한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주어진 값 x(i)의 연속항에 대해, 전체 로그 우도는 다음과 같다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i에 의존하지 않는 항목들을 밖으로 꺼내면 다음과 같은 식이 나온다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 식을 파이썬으로 변환하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class Height\n",
    "    \n",
    "    def LogUpdateSetFast(self, data):\n",
    "        xs = tuple(data)\n",
    "        n = len(xs)\n",
    "        \n",
    "        for hypo in self.Values():\n",
    "            mu, sigma = hypo\n",
    "            total = Summation(xs, mu)\n",
    "            loglike = -n*math.log(sigma) - total / 2 / sigma**2\n",
    "            self.Incr(hypo, loglike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 자체로도 약간의 개선이 있지만 더 개선을 원한다면 <br />\n",
    "덧셈은 mu에만 좌우될 뿐, sigma와는 상관이 없다는 것을 기억하면, mu의 값만 계산해도 된다는 것을 알 수 있다. <br />\n",
    "다시 계산하는 것을 피하기 위해, 덧셈을 하는 함수를 만든 후 메모이즈해서 사전에 게산된 결과가 딕셔너리에 저장되도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Cf) 메모이제이션\n",
    ">  컴퓨터 프로그램이 동일한 계산을 반복해야 할 때, 이전에 계산한 값을 메모리에 저장함으로써 <br />\n",
    "> 동일한 계산의 반복 수행을 제거하여 프로그램 실행 속도를 빠르게 하는 기술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Summation(xs, cache={}):\n",
    "    try:\n",
    "        return cache[xs, mu]\n",
    "    except KeyError:\n",
    "        ds = [(x-mu)**2 for x in xs]\n",
    "        total = sum(ds)\n",
    "        cache[xs, mu] = total\n",
    "        return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cache는 전에 계산된 합을 저장한다. try 구문은 가능한 경우 캐시의 값을 반환하고, 아닌 경우 합을 계산한 후 캐시에 넣고 결과를 반환한다. <br />\n",
    "여기서 유일한 단점은 캐시에서는 키로 리스트를 사용할 수 없다는 점인데, 이는 해시가 가능한 타입이 아니기 때문이다. <br /> 그래서 LogUpdateSet에서 데이터를 튜플로 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (8) 근사 베이지안 계산(ABC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABC(Approximate Baysian Computation)은 어떤 특정 데이터셋의 우도가 다음과 같을 때 필요하다. <br />\n",
    "- 특정 큰 데이터셋의 경우, 매우 작아서 로그 변환이 어려운 경우\n",
    "- 계산 비용이 많이 들어서 최적화를 많이 해야 하는 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "우리는 우리가 본 정확한 데이터셋을 보는 것에 대한 우도가 어떻든 별로 상관없다. <br />\n",
    "특히 연속형 변수의 경우, 우리가 본 것과 같은 모든 데이터셋을 보게 되는 것에 대한 우도에만 신경 쓴다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를들어, 유로 문제에서 우리는 동전을 던진 순서에 대해서는 전혀 신경 쓰지 않고, 앞면과 뒷면의 전체 수에만 관심을 두었다. <br />\n",
    "기차 문제의 경우 어떤 특정 기차를 보았는지는 상관없이 기차의 수와 번호의 최댓값에만 신경을 썻다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 이렇게 묻는게 더 정확할 것이다. <br />\n",
    "\"만약 mu와 sigma를 가설 값으로 갖는 인구 중 100,000명의 사람을 샘플링하면 <br /> \n",
    "관측 평균과 분산에 대한 샘플을 모을 수 있는 확률이 얼마나 될까?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가우시안 분포에서 샘플링하면 이미 샘플 통계에서 분포를 정확히 찾아낼 수 있기 때문에 보다 효과적으로 답을 알아낼 수 있다. <br />\n",
    "만약 변수 mu와 sigma를 사용하는 가우시안 분포에서 n개의 값을 뽑아내서 샘플 평균 m을 구한 경우 <br />\n",
    "m의 분포는 mu와 sigma / root(n)을 변수로 사용하는 가우시안 분포를 따른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "유사하게, 샘플 표준편차 s의 분포는 sigma / root(2(n-1))의 변수를 취하는 가우시안 분포다. <br />\n",
    "이 샘플 분포를 사용해서 mu와 sigma의 가설 값이 주어졌을 때 샘플 통계의 우도를 계산 할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음은 이를 실행하는 LogUpdateSet의 새 코드다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def LogUpdateSetABC(self, data):\n",
    "        xs = data\n",
    "        n = len(xs)\n",
    "        \n",
    "        # 샘플 통계 계산\n",
    "        m = numpy.mean(xs)\n",
    "        s = numpy.std(xs)\n",
    "        \n",
    "        for hypo in sorted(self.Values()):\n",
    "            mu, sigma = hypo\n",
    "            \n",
    "            # 주어진 hypo에 대해 m의 로그 우도를 계산\n",
    "            \n",
    "            stderr_m = sigma / math.sqrt(n)\n",
    "            loglike = EvalGaussianLogPdf(m, mu, stderr_m)\n",
    "            \n",
    "            # 주어진 hypo에 대해 s의 로그 우도 계산\n",
    "            \n",
    "            stderr_s = sigma / math.sqrt(2*(n-1))\n",
    "            loglike += EvalGaussianLogPdf(s, sigma, stderr_s)\n",
    "            \n",
    "            self.Incr(hypo, loglike)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (9) 로버스트 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터셋에는 오류일 가능성이 많은 아웃라이어가 많이 있다. <br />\n",
    "예를 들어, 키가 61cm로 기록된 성인 세 명이 있는데, 이는 아마도 세상에서 가장 단신인 성인일 것이다. <br />\n",
    "반면에, 키가 229cm로 기록된 여성이 네 명 있는데, 이는 세상에서 가장 장신인 여성들일 것이다. <br />\n",
    "이 값들이 사실일 가능성도 있기는 하지만, 이런 극단적인 값은 변이 추정에 있어서 불균형적 영향을 미치기 때문에 바로잡아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABC는 요약 통계 기반이므로 전체 데이터셋을 다루는 것에 비해 요약 통계를 선택하는 것이 더 아웃라이어에 대한 신뢰도를 높인다. <br />\n",
    "예를 들어, 샘플의 평균과 표준편차 대신, 중간값과 25번째와 75번째 백분위수의 차잇값인 4분위 범위를 사용할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "좀 더 일반적으로 분포 p의 주어진 어느 범위 내의 차잇값을 구하는 분위 내 범위를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MedianIPR(xs, p):\n",
    "    cdf = thinkbayes2.MakeCdfFromList(xs)\n",
    "    median = cdf.Percentile(50)\n",
    "    \n",
    "    alpha = (1-p) / 2\n",
    "    ipr = cdf.Value(1-alpha) - cdf.Value(alpha)\n",
    "    return median, ipr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xs는 연속값이다. p는 필요한 범위로, 예를 들자면 p = 0.5는 4분위 범위가 된다. <br />\n",
    "MedianIPR은 xs의 CDF를 계산해서 중간값과 두 분위 수의 차이를 찾는다.<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가우시안 CDF를 사용해서 ipr을 sigma의 추정값으로 변환해서 표준편차의 주어진 수로 주어진 범위 내의 분포를 계산한다. <br />\n",
    "예를 들어, 가우시안 분포의 68%가 표준편차 1 이내의 중간에 차지하고, 양쪽 끝에 16%가 있다는 잘 알려진 경험적 법칙이다. <br />\n",
    "만약 16번째와 84번째 백분위값 사이의 범위를 계산한다면 결과가 2*sigma가 나온다는 것을 알 수 있다. <br /> \n",
    "따라서 68% IPR을 계산한 후 2로 나누는 식으로 sigma를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보다 일반적으로 sigmas의 어떤 숫자든 사용하게 할 수 있다. MedianS는 이런 더 일반적인 방식으로 계산을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    def MedianS(xs, num_sigmas):\n",
    "        half_p = thinkbayes2.StandardGaussuian(num_sigmas) - 0.5\n",
    "        \n",
    "        median, ipr = MedianIPR(xs, half_p*2)\n",
    "        s = ipr / 2 / num_sigmas\n",
    "        \n",
    "        return median, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xs는 값의 연속형이고, num_sigmas는 기반이 되는 결과의 표준편차 숫자다. <br />\n",
    "결과는 mu를 추정하는 median과 sigma를 추정하는 s다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 LogUpdateSetABC에서 샘플 평균과 표준편차를 median과 S로 치환할 수 있다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mu와 sigma를 추정하기 위해 관측 백분위수를 사용하는게 좀 이상하다고 느껴질 수 있겠지만 <br />\n",
    "이는 베이지안 접근법의 유연성을 설명하기 위한 예가 될 수 있다. 예를 들어, 이렇게 질문 할 수 있다. <br />\n",
    "\"mu와 sigma에 대한 가설 값과 오차가 나타내는 샘플링 프로세스가 주어졌을 때 샘플 통계의 주어진 집합을 만드는 우도는 얼마일까?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우리는 원하는 샘플 통계를 어떤 것이든 사용할 수 있다. <br />\n",
    "mu와 sigma의 위치와 분포의 퍼진 정도를 결정하므로 이런 성격을 가진 통계값을 선택해야 한다. <br />\n",
    "예를 들어, 49번째와 51번째 백분위 수를 선택한다면 분포의 퍼짐 정도에 대한 정보는 거의 얻을 수 없으므로 <br />\n",
    "데이터에 대해 상대적으로 제한을 받지 않는 sigma값을 추정할 때 사용한다. <br /> \n",
    "sigma의 모든 값은 관측값을 만드는 때에 거의 동일한 우도를 가지므로 sigma의 사후 본포는 사전 분포와 상당히 유사하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (10) 누가 더 변이성이 높은가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그래서 남자와 여자 중 누가 더 변이 계수가 큰가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_sigmas = 1 일 때 중간값과 IPR기반의 ABC를 사용해서, mu와 sigma 기반의 사후 결합 분포를 계산한다.<br />\n",
    "다음의 다이어그램들은 x축으로 mu를 놓고, y축으로 sigma를 놓고, z축에 확률을 놓은 등고선 그래프를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미국 남성의 키의 평균과 표준편차에 대한 사후 결합 분포의 등고선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미국 여성의 키의 평균과 표준편차에 대한 사후 결합 분포의 등고선"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 결합 분포에 대해 CV의 사후 분포를 계산했다. 아래의 다이어그램은 남녀에 대해 이런 분포를 보여준다. <br />\n",
    "남성의 평균은 0.0410, 여성의 평균은 0.0429다. <br /> \n",
    "이 분포 간에 겹치는 부분이 없으므로 여성이 키에 있어서는 남성보다 변이성이 더 높다는 결론을 확실하게 낼 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 이 결과는 분위 내 범위 선택에 따라 달라진다는 것으로 밝혀졌다. <br />\n",
    "num_sigmas = 1 일때는 여성이 더 변이성이 높다는 결론을 얻을 수 있지만, <br />\n",
    "num_sigmas = 2의 경우 남성이 더 변이성이 높다는 가설이 동일한 신뢰도를 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이런 차이가 발생하는 원인은 작은 쪽에 치우친 경우가 남성이 더 많고 따라서 평균으로 부터의 거리도 커진다. <br />\n",
    "따라서 변이 가설 평가는 변이 해석에 따라 달라진다. <br />\n",
    "num_sigmas = 1 일 때 사람들이 평균 부근에 많다는 점에 집중하자. <br />\n",
    "num_sigmas가 증가할수록, 극단값의 비중이 높아진다. <br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "무엇을 강조하는 것이 적합하는지 결정하기 위해서는 가설을 더욱 정확히 파악하는 것이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (11) 토의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABC에 대해서 두 가지 방식으로 생각해 볼 수 있다. <br />\n",
    "첫 번째 해석은 이름의 의미대로, 원값보다 빠르게 계산하기 위해 사용하는 추정값이다. <br />\n",
    "하지만 베이지안 분석이 항상 모델링 기반임을 떠올려 보면, 어차피 정확한 해답은 없다는 뜻이다. <br />\n",
    "많은 흥미로운 물리 시스템은 여러 가능한 모델을 갖고, 각 모델은 서로 다른 결과를 낸다. <br />\n",
    "이 결과를 해석하기 위해 모델을 평가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABC의 다른 해석으로는 \"주어진 가설하에서 데이터의 우도는 어떻게 되는가?\"라는 질문에 대한 대답으로 <br />\n",
    "p(D|H)계산을 위한 위도의 대안 모델이라는 것이다. <br />\n",
    "큰 데이터셋의 경우, 데이터의 우도는 매우 작아지고, 이는 제대로 된 질문을 할 수 없을 수도 있다는 것을 알려준다. <br />\n",
    "우리가 정말로 알고자 하는 것은 데이터 같은 결과에 대한 우도로, 이 때 \"같은\"의 정의는 다른 모델을 결정하는 것일 수도 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ABC의 기저에 깔린 개념은 두 데이터셋이 동일한 요약 통계치를 가질 때는 거의 동일하다는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ { x }_{ 1 } $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
