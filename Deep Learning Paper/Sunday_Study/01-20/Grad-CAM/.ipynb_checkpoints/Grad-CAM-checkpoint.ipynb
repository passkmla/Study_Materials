{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class Activation의 Generalize Version 이라고 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why interpretability matters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The lack of decomposability of deep network into intuitive and understandable components makes them hard to interpret\n",
    "- Transparent model is necessary to bulid trust in intelligent systems and move towards into our everyday life\n",
    "- When AI is weaker to identify failure modes\n",
    "- When AI is on par to establish trust and confidence in users\n",
    "- When AI is stronger Machine teaching a human to make better decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CAM: Learning deep features for discrimitive localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class Activation Mapping is applicable to only GAP layers\n",
    "- Make CAM to applicable to a wide variety of CNN models <br />\n",
    ": CNNs with fully-connected layers (e.g VGG) <br />\n",
    ": CNNs for structured outputs (e.g. captioning) <br />\n",
    ": CNNs used in tasks with multi-modal inputs (e.g VQA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualizing CNNs <br />\n",
    "Highlight important pixels: non discriminative <br />\n",
    "Synthesize images to maximally activate a network unit or invert a latent representation: not for specific input images\n",
    "- Assessing Model Trust <br />\n",
    ": Motivated by notions of interpretability <br />\n",
    ": There are some methods to assess trust in models\n",
    "- Weakly supervised localization <br />\n",
    ": Pertubing inputs by occlusion <br />\n",
    ": Marginal winning probability <br />\n",
    ": Class Activation Mapping(Global Average Pooling, Global Max Pooling, log-sum-exp pooling) <br />\n",
    ": This paper introduces a new way of combining feature maps using the gradient signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Apply Grad-CAM to any CNN-based network without requring architectural changes or re-training\n",
    "- Apply Gram-CAM to existing top-performing classification, captioning, anc VQA\n",
    "- Conduct human studies if it helps establish human trust and untrained user can discern a stronger network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/ramprs/grad-cam/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=COjUB9Izk6E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-CAM as a generalization of CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Weakly supervised localization <br />\n",
    ": Use off-the-shelf VGG-16 from Caffe Model Zoo <br />\n",
    ": Binarize Grad-CAM with 15% of the max indensity <br />\n",
    ": Draw bounding box around the single largest segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Weakly supervised segmentation <br />\n",
    ": Replace CAM with Grad-CAM in Seed, Expand, Constrain(SEC) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pointing Game <br />\n",
    ": Grad-CAM (70.58%) vs C-MWP(60.30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class Discrimination <br />\n",
    ": 43 AMT workers, 4 visualizations, 90 image category pairs, 9 rating each <br />\n",
    ": Deconv vs Guided backprop vs Guided Grad-CAM vs Deconv Grad-CAM <br />\n",
    ": 53.33% vs 44.44% vs 61.23% vs 61.23%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trust worthiness <br />\n",
    ": 54 AMT workers, 2 classifiers(AlexNet, VGG-16), 2 visualizations <br />\n",
    ": Show same prediction with similar output score <br />\n",
    ": Human can identify VGG-16 is better <br />\n",
    ": Guided Grad-CAM show higher difference <br />\n",
    ": 1.27(vs 1.0 with Guided Backprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Faithfulness vs Interpretability <br />\n",
    ": CNN scores after occlude image patches <br />\n",
    ": Score correlates highly with Grad-CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Class discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=200 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trust worthiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnosing image classification CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Analyzing failure modes for VGG-16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/9.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Identifying bias in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/10.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/11.png\" width=250 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use negative values to find regions that decrease output score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Use neuraltalk2: VGG-16 for image and LSTM language model (No explicit attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Compare with DenseCap <br />\n",
    ": Consist of Fully Convolitional Localization Network and LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./Images/13.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visual Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Compare with human attention map <br />\n",
    ": Correlation 0.136 on 1374 val question-image pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"./Images/14.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- The paper proposed Gradient-weighted Class Activation Mapping as a generalization of CAM\n",
    "- Combined Grad-CAM with existing high-resolution visualzations\n",
    "- Human studies reveal the trustworthiness of a classifier, and help identify biases in datasets\n",
    "- AI system should not only be inteliigent, but also be able to reason about its beliefs and actions for human to trust it\n",
    "- Future work includes other networks such as reinforcement learning, natural language processing, and video applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
