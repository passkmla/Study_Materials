{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A simple modification of the GAP(Global Average Pooling) layer + Class Activation Mapping(CAM)\n",
    "- Localization the discriminative image regions using CAM in a single forward-pass\n",
    "- Achieve 37.1% top-5 error for object localization on ILSVRC 2014 <br />\n",
    "(34.2% top-5 test error acheived by fully supervised AlexNet)\n",
    "- A generic localizable deep representation that can be applied to a variety tasks\n",
    "(transferred to other recognition datasets for generic classification, localization concept discovery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Average Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Network in Network(NIN)에서 처음 제안됨\n",
    "- Fully-connected layers의 사용이 overfitting을 일으킴\n",
    "- Structural regularizer: Parameter 수를 줄여 overfitting을 막기 위해 FC layer 대신 사용됨\n",
    "- GAP는 spatial information을 채널별로 sum하기 때문에 input의 spatial translation에 더 robust 하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weakly-supervised Object Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Not end to end system\n",
    "- Require multiple forward passes of a network\n",
    "- Global max pooling (point localization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- End to end, single forward pass, global average pooling\n",
    "- GAP 자체보다는 accurate discriminative localization에 적용될 수 있다는 observation이 contribution이라고 주장함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fully connected layer을 거치면서 좋은 localization 능력이 무시되고, incomplete activation map을 얻게 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제안된 방법은 fully connected layer 대신 Global Average Pooling을 이용하면 performance를 유지하면서 network를 처음부터 끝까지 이해 가능하다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Activation Mapping(CAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global average pooling(GAP) vs global max pooling(GMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GAP loss encourage the network to indentify extent of the object\n",
    "- GMP loss encourage the network to indentify just one discriminative part\n",
    "- GAP는 모든 discriminative part를 고려해서 map의 score에 영향을 주지만, GMP는 most discriminative one을 제외한 low score들은 전체 score에 임팩트를 주지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AlexNet, VGGNet, GoogLeNet을 변형\n",
    "- Fully connected layer를 GAP로 바꿈\n",
    "- Mapping Resolution을 올리기 위해 여러 conv layer들을 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify\n",
    "- AlexNet: conv 5 뒤쪽 레이어 제거, mapping resolution: 13x13 -  AlexNet-GAP\n",
    "- VGGNet: conv 5-3 뒤쪽 레이어 제거, mapping resolution: 14x14 -  VGGNet-GAP\n",
    "- GoogLeNet: inception4e 뒤쪽 레이어 제거, mapping resolution: 14x14 -  AlexNet-GAP\n",
    "- 마지막에 3x3, stride 1, pad 1 with 1024 units conv net 추가 후\n",
    "GAP layer + softmax\n",
    "- Fine-tuning on ILSVRC dataset(1000 class)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original AlexNet, VGGNet, GoogLeNet, NIN과 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localization 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original GoogLeNet, NIN, backpropagation instead of CAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet-GAP와 GoogLeNet-GMP 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A small performance drop of 1~2% when removing conv layer.\n",
    "- Localization에 대한 높은 성능을 얻기 위해서는 classification 성능이 중요함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localization Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bounding box 생성: Simple thresholding technique to segment heatmap (Above 20% of the max value of the CAM, largest connected component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/10.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Features for Generic Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher-level layer + linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/12.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-grained Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data set: 200 bird species, 11,788 images, Bbox Anotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/13.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discovering informative objects in the scenes\n",
    "- 10 scene categories from SUN data(total 4675 fully annotated images)\n",
    "- GAP layer(GoogLeNet-GAP) + one-vs-all linear SVM\n",
    "- Informative object: 가장 자주 나타나는 Top6 objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/14.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concept localization in weakly labeled images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/15.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Weakly supervised text detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/16.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Interpreting visual question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/17.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class specific unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/18.png\" width=500 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
