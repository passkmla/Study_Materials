{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM은 “Support Vector Machine”의 약어로 Support Vector와 hyper-plane이 주요 개념인 머신러닝 알고리즘의 하나이다. <br /> \n",
    "SVM은 분류(classification)나 회귀 분석(regression)에 사용이 가능하며, 특히 분류 쪽의 성능이 뛰어나기 때문에 주로 분류에 많이 사용이 된다. <br /> \n",
    "SVM은 기본적으로 지도 학습 알고리즘이며, hyper-plane(초 평면)을 이용해 카테고리를 나눈다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear separability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림은 선형적으로 분류가 가능한 경우와 그렇지 못한 경우를 보여준다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector와 Hyper-plane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림의 예처럼, 빨간 삼각형과 파란 점으로 구성된 집단을 나눈다면 어느 것이 가장 최적으로 분류한 것일까? <br /> \n",
    "이 부분 역시 대부분 직관적으로 오른쪽 아래에 있는 분류가 가장 잘 된 것이라고 판단할 것이다. <br />\n",
    "최적으로 나누는 것은 어떻게 할 수 있을까?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=500 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 해답에 SVM의 답이 있다. <br /> \n",
    "SVM은 분류를 할 때 최고의 마진을 가져가는 방향으로 분류를 수행한다. <br /> \n",
    "마진이 크면 클수록 학습에 사용하지 않은 새로운 데이터가 들어오더라도 잘 분류할 가능성이 커지기 때문이다. <br /> \n",
    "이것은 SVM 뿐만 아니라 모든 머신러닝 학습 방법이 추구하는 방향이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림을 보자. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림에서 파란색 점과 빨간색 점을 최대 마진으로 구별하는 직선은 그림에서 검은색 직선이다. <br /> \n",
    "그림의 예는 2차원이기 때문에 직선이지만, 3차원 이상이 되면 평면이 되며 이것을 초 평면(hyper-plane)이라고 부른다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 hyper-plane으로부터 가장 가까이 있는 파란색 점과 빨간색 점을 Support vector라고 부른다. <br /> \n",
    "SVM이라는 이름도 여기서 기원한 것이다. <br /> \n",
    "마진의 폭은 $\\frac { 2 }{ ||W|| }$ 이며, 마진이 최대가 되려면, 결과적으로 $||W||$가 최소가 되도록 최적화를 해줘야 하며, 이 과정이 바로 SVM 최적화 과정이 된다. <br /> \n",
    "$||W||$가 최소가 된다는 뜻은 결과적으로는 ${||W||}^{2}$이 최소가 되는 것이기 때문에 quadratic optimization 과정으로 볼 수 있게 된다. <br /> \n",
    "이 quadratic optimization은 대부분의 머신러닝 알고리즘들이 최적화 방법으로 사용을 하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slack 변수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 두 가지 경우는 어떤 경우가 최적일까?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "왼쪽은 두 개의 그룹을 선형적으로 분리를 하기는 잘 했지만 마진이 거의 없다. <br /> \n",
    "마진이 거의 없다면 학습 데이터에 대한 학습은 잘 했을지 몰라도, 학습에 사용되지 않은 새로운 데이터가 들어올 경우 에러가 발생할 가능성이 높아진다. <br />\n",
    "반면에, 오른쪽 그림은 파란점 1개에 대해 분류에 대한 오류가 있기는 하지만, 마진은 매우 크다. <br /> \n",
    "비록 학습 데이터에 대해서는 1개의 오류가 있을지 몰라도 (그 점이 특이점이라면 에러가 있는 것이 낫다), 마진이 충분하기 때문에 새로운 데이터에 대한 예측은 더 낫다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과연 어떤 분류가 최적일까? <br /> \n",
    "일반적으로 마진과 학습 오류의 개수는 trade-off 관계에 있으며, 어떻게 분류할 것인지는 목적에 따라 결정을 해줘야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다시, 아래 그림을 보자.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slack 변수 ξ 란 선형적으로 분류를 할 수 없는 경우에 분류를 위해 오차를 허용해야 하는데, 이 때 constraints를 완화하여 오차(violation)를 허용할 때 사용하는 변수이다. <br /> \n",
    "단순한 함수를 이용해 데이터를 학습 시키게 되면, 학습이 어렵거나 overfitting 현상이 발생할 수 있는데 이것을 피하기 위해 고안된 개념이다.\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slack 변수 ξ 값에 따른 분류를 하면 다음과 같다. <br /> \n",
    "ξ 가 0이면 정상적으로 분류된 경우이고, 0 < ξ < 1 이거나 ξ = 1이면 이면 마진이 적은 margin violation 경우이며, ξ > 1이면 분류가 잘못된 경우를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM 최적화를 수행할 때, slack 변수까지 고려하면 아래와 같은 식으로 표현이 가능하다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/10.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 식에서 $C$를 Regularization parameter라고 부른다. <br /> \n",
    "Regularization이라는 말은 학습 시에 학습 데이터에만 너무 특화되면, 학습에 사용하지 않은 새로운 변수가 들어 왔을 때 예측이 잘못되는 것을 막기 위한 <br /> \n",
    "일반화(regularization) 목적으로 들어가는 일종의 penalty 항이라고 생각할 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C$ 값이 큰 경우는 허용 오차의 개수가 작아야 하기 때문에 오직 $||W||$에 집중하는 경향이 있으므로 margin이 좁아지는 경향을 보이며, $C$가 무한대인 경우는 <br /> \n",
    "학습 오차가 생기면 안 되는 상황이며, $C$가 작은 경우는 margin이 커지게 된다. <br />\n",
    "이처럼 $C$값을 사용해 margin 폭을 유연하게 조절할 수가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비선형 특징들에 대한 classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM의 목표는 고차원의 특징 공간(feature space)에서 데이터들을 잘 구별할 수 있는 초 평면(hyper-plane)을 학습을 통하여 결정을 하며, 연산량 관점에서 <br /> \n",
    "효율적으로 구할 수 있는 방법을 제공하는 것이다. <br />\n",
    "하지만, 아래 왼쪽 그림과 같이, 선형적으로는 도저히 구별하기가 어려운 특징들에 대해서도 초 평면을 구할 수 있는 방법이 있을까?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다행스러운 것은 선형적으로는 구별하기 어려운 특징들을 위 그림의 오른쪽처럼 구별할 수 있는 방법들이 있다는 것이다. <br /> \n",
    "아래 그림처럼, 구별이 가능한 방향으로 사상(mapping)을 시키면 원래는 선형적으로 구별이 불가능한 특징들도 새로운 공간에서는 구별이 가능하게 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Trick (Kernel function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림의 왼쪽처럼 2차원 평면 상에서 동그라미와 X가 있는 경우를 생각해보자. <br /> \n",
    "이를 구별하려면 왼쪽 그림처럼, 타원을 만들어 구별을 할 수가 있다. <br />\n",
    "하지만, 이것을 mapping 함수를 통해 오른쪽과 같은 3차원 평면으로 변환시킬 수가 있다면, 선형적으로 구별이 가능해진다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/8.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 저 차원에서는 선형적으로 구별이 불가능하지만, mapping을 통해 고 차원으로 변환시킨 후 선형적으로 구별이 가능하도록 하는 방법을 kernel trick이라고 부르고, <br /> \n",
    "그 때 사용하는 함수를 kernel 함수라고 부른다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원래 SVM은 특징 벡터(feature vector)에 주목을 하였지만, 특징 벡터를 사용하는 것보다는 kernel 함수를 사용하는 편이 훨씬 간결하게 표현이 가능하고, <br />\n",
    "비선형적인 특징들도 구별이 가능하기 때문에 kernel trick이 고안된 것이다. <br />\n",
    "Kernel 함수를 사용할 때의 이점은 최적화에 관련된 복잡도가 새로 옮겨진 특징 공간의 복잡도에 영향을 받는 것이 아니라 이전 입력 공간의 영향을 받는다는 점이기 때문에, 설사 mapping을 통해 무한대의 공간으로 변환이 된다고 할지라도 최적화에 문제가 없다. <br />\n",
    "최적화에는 선형적인 경우와 마찬가지로 라그량주 곱셈기(Lagrange Multiplier) 방법을 사용하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "흔히 사용되는 kernel 함수는 아래와 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/9.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Support Vector Machines – Kernels and the Kernel Trick](http://www.cogsys.wiai.uni-bamberg.de/teaching/ss06/hs_svm/slides/SVM_Seminarbericht_Hofmann.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MIT OCW SVM](https://www.youtube.com/watch?v=_PwhiWxHK8o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
