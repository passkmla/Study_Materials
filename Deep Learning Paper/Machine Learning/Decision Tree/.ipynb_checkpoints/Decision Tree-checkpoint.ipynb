{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree는 나무 모양의 그래프를 사용하여 최적의 결정을 할 수 있도록 돕는 방법(알고리즘)이기 때문에 기회 비용에 대한 고려, 기대 이익 계산, <br />\n",
    "자원의 효과적인 사용이나 위험 관리 등 효율적인 결정이 필요한 많은 분야에 사용이 되고 있다. <br />\n",
    "머신 러닝에서 사용하는 ‘Decision Tree Learning’ 혹은 그냥 ‘Decision Tree’라고 부르는 기법은 Decision Tree 방법을 머신 러닝에 적용한 것을 말하여, <br />\n",
    "어떤 항목에 대한 관측값(observation)에 대하여 가지(branch) 끝에 위치하는 기대값(target)과 연결시켜 주는 예측 모델(predictive model) 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree의 3대 요소(노드, 가지, 잎)를 살펴보자. <br /> \n",
    "아래 그림은 흔히 사용되는 “테니스를 칠 수 있을까?” 예제이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "각 노드는 1개의 속성(attribute) $X_i$를 판별하는 역할을 한다. <br /> \n",
    "각 가지(branch)는 각 노드로부터 나오며, 속성 $X_i$에 대한 1개의 값을 갖는다. <br /> \n",
    "잎(Leaf node)은 최종단에 있으며, $X_i$일 때의 그것에 대응하는 기대값 $Y$에 해당된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제에서 속성이란 습도(Humidity), 날씨(Outlook), 바람(Wind)과 같이 테니스를 치기에 적합한지 여부를 나타내는 주요한 ‘판단의 기준’이며, <br />\n",
    "이 판단의 기준에는 목적에 따라 많은 것들이 올 수가 있다. <br />\n",
    "속성에는 여러 개의 값이 있을 수 있으며, 위 예제의 Outlook에는 Sunny, Overcast 및 Rain 총 3개의 값이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Learning 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림은 타이타닉 호에서의 생존 여부를 나타내는 ‘예제’이며, 각 노드는 생존 여부 결정에 필요한 중요한 속성이며 가지에는 속성에 대한 판단 결과가 나타나며, <br /> \n",
    "가지에는 또 다른 노드가 올 수도 있고 잎(leaf)이 올 수도 있다. <br /> \n",
    "잎에는 최종 결과(기대값, 생존 또는 사망)가 오며, 이 예제에서는 기대값에 딸린 두 개의 숫자가 있는데 하나는 그 잎(예를 들어, 맨 오른쪽의 ‘생존’이라면, 남자가 아닌 경우)에서의 <br /> \n",
    "생존확률이고 다른 하나는 전체 중 그 잎에 해당될 확률을 말한다. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 root node는 ‘남자’라는 속성이 왔으며, 가지에서 ‘아니오’를 따라가면 ‘생존’이라는 잎이 나온다. <br /> \n",
    "남자가 아닌 경우에 해당될 확률은 36%이고 생존 확률은 0.73이라는 것을 알 수 있다. <br />\n",
    "‘예’를 따라 가면, 다시 ‘나이 > 9.5’라는 속성이 나온다. <br /> \n",
    "이 노드에서 ‘예’ 쪽을 따라가면, 남자이고 나이가 9.5세 이상에 해당하는 경우가 61%이고 대부분 사망이며, 그 중 생존 확률은 0.17이다. <br />\n",
    "위와 같은 방식으로 결정이 되기 때문에, 어떤 특정 대상이 정해지면 사망과 생존 확률을 예측(판단) 할 수 있게 되는 것이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 굉장히 직관적이고 편리하게 예측(결정) 모델을 만들 수 있고, 새로운 데이터에 대하여 쉽게 판단할 수 있기 때문에 많이 사용이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree 만들기와 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree를 구성하는 방법은 여러 가지가 있을 수 있다. 특히 속성이 여러 개 있는 경우는 어떤 속성을 root node에 둘 것인지는 매우 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 Decision Tree를 구성할 때의 목표는 더 compact 하게 만드는 것이며, Compact 하게 만들기 위한 방법으로 엔트로피를 이용한다. <br /> \n",
    "엔트로피란 데이터의 분포의 순도(purity)를 나타내는 척도이며, 데이터의 순도가 높을수록 엔트로피의 값은 낮아지고, 많이 섞이면 섞일수록 엔트로피의 값이 커지게 되며, <br />\n",
    "아래 식처럼 정의를 할 수가 있다. <br /> \n",
    "엔트로피를 높게 하면, Decision Tree의 구성은 더욱 compact 해진다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=300 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 식에서 $P_i$는 특정 값 $i$가 일어날 확률을 의미한다. <br /> \n",
    "위 식은 간단하지만, 어떻게 사용하는지 살펴보기 위해 엔트로피를 구하는 몇가지 경우를 살펴보자.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약에 3개는 Yes이고 2개는 No인 경우라면, 엔트로피는  E = -(2/5)log2(2/5) – (3/5)log2(3/5) = 0.971 이다. <br />\n",
    "4개는 Yes이고, 0개는 No인 경우는 E = -(4/4)log2(4/4) –(0/4)log2(0/4) = 0으로 순도가 100%인 경우는 엔트로피는 0이 된다. <br /> \n",
    "반면에 2개 Yes이고, 2개는 No인 경우라면 엔트로피는 1로 최대가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이처럼 엔트로피는 class의 분포가 고를수록 큰 값을 같고, 특정 값으로 몰려있게 되면 0이 된다. <br /> \n",
    "아래 그림은 class가 2개인 경우에 데이터의 분포에 따른 엔트로피 변화를 보여주는 그림이다. <br /> \n",
    "고르면 고를수록 코딩에 필요한 bit 효율이 올라가게 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=250 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree 만들기 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래와 같은 조합이 있을 경우 Decision Tree를 어떻게 만들까?\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/6.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 표에는 총 14일 동안 골프를 치거나 치지 않거나 한 경우의 수를 보여주고 있다. <br /> \n",
    "이것을 보고 한 눈에 특정 경우에 골프를 칠 것인지 말 것인지를 판단하는 것은 쉽지 않을 것이다. <br /> \n",
    "이럴 경우 Decision Tree를 사용하면 깔끔하게 정리할 수 있으며, 골프를 칠 것인지 아닌지를 쉽게 예측할 수 있게 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 예제에서 4개의 속성은 Temperature, Outlook, Humidity, Windy이다. <br />\n",
    "이것을 바탕으로 최적의 Decision Tree를 만들려면, 각각의 속성들에 대한 Entropy를 계산해야 한다. <br />\n",
    "Entropy를 앞서 살펴본 계산식에 따라 계산했더니, Outlook이 최고의 Entropy가 나왔기 때문에 Outlook을 root node로 결정을 한다. <br />\n",
    "이제 Outlook이 정해졌으니, Outlook 속성에 있는 3가지 값 Sunny, Overcast, Rain을 가지로 사용하여 똑 같은 방식으로 다음 노드를 결정한다. <br />\n",
    "결과적으로 아래와 같은 Decision Tree를 얻을 수 있게 된다. \n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/7.png\" width=500 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree에서의 Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting이란 학습 데이터에 대해서는 좋은 결과를 내지만, 학습에 사용하지 않은 실제 테스트 데이터에 대한 결과는 오히려 더 나빠지는 것을 말한다. <br /> \n",
    "이는 통상적으로 학습 데이터가 부족하거나, 학습 데이터에 잡음이 있는 경우에 학습 데이터에 너무 특화되어 일반화(generalization) 특성이 떨어지면서 생기는 문제이다. <br /> \n",
    "그래서 어느 머신러닝 방법을 사용하더라도 Overfitting으로 인한 문제를 완전히 피해가기는 어려우며, 학습 방법에 맞춰 적절한 Overfitting을 회피하기 위한 기법이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree를 적용하여 학습을 하는 경우에도 Overfitting 문제가 발생할 수 있다. <br />\n",
    "아래 그림은 Decision Tree를 설명하는 거의 대부분의 자료에서 Overfitting을 설명하는데 사용되는 그림이다. <br /> \n",
    "Tree의 크기가 대략적으로 23 이상이 되면 test data에 대한 정확도가 점점 감소하는 것을 확인할 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/8.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree 방식에서는 Tree가 커지면 커질수록 점차 세밀한 분류가 가능해지지만, 반대로 Tree가 커지면 커질수록 학습 데이터에 특화될 가능성이 커진다는 것을 <br />\n",
    "위 그림을 통하여 유추할 수가 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting 예"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Decision Tree Overfitting](http://www3.nd.edu/~rjohns15/cse40647.sp14/www/content/lectures/24%20-%20Decision%20Trees%203.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "포유류를 구별하는 학습 데이터에 잡음으로 인해 tag가 엉뚱하게 달려 있는 학습 데이터를 사용하는 경우를 살펴보자. <br /> \n",
    "아래 표에서 *가 붙은 데이터는 분류가 잘못된 경우이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/9.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잡음으로 인해 엉터리 tag가 붙은 학습 데이터를 이용해 아래와 같은 Decision Tree를 만들었다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/10.png\" width=400 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 Decision Tree를 만들고 나서, 아래 표와 같은 테스트 데이터를 이용하여 실험을 한다면, 사람(human)과 돌고래(dolphin)를 포유류가 아니라고 분류를 하게 된다. <br />\n",
    "이것은 명백한 오류이다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/11.png\" width=600 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그렇다면, 어떻게 하면 Overfitting을 피할 수 있을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting을 피하는 방법 – 가지치기(pruning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting을 피하는 방법으로 흔히 사용되는 말이 “Occam’s Razor” 이다. <br /> \n",
    "학습 데이터에 너무 특화되어 일반성을 잃는 것을 경계하기 위해 머신러닝에서 학습 모델을 결정할 때 흔히 사용이 된다. <br /> \n",
    "Decision Tree에서는 앞서 살펴본 예의 경우처럼, 잡음이 있는 데이터에 너무 특화 되어 tree가 길어지게 되면 오히려 엉뚱한 결과가 나올 수도 있다. <br />\n",
    "앞서 살펴본 예를 아래처럼 단순하게 만들면, 일반화의 특성이 좋아지면서, test error가 더 줄어들게 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/12.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 tree를 줄이면 학습 데이터에 대한 error가 발생하기는 하지만, test 데이터에 대한 error가 줄어들게 된다. <br /> \n",
    "이것을 가지치기(pruning)이라고 한다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지나치게 세분화 되는 것을 피하기 위해, 마치 가지치기를 하듯이 특정 가지(node) 이후를 잘라내는 방법이다. <br />\n",
    "이를 위해 학습 데이터를 이용해 학습을 시킨 후 학습의 결과를 검증하기 위한 별도의 검증 데이터(validation data)를 이용해 학습 결과의 특화 현상 여부를 판단하고, <br /> \n",
    "pre-pruning과 post-pruning을 적절하게 섞어 사용한다. Pruning을 사용하면 아래와 같은 결과를 얻을 수 있다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/13.png\" width=500 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree 장점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 빠르게 구현이 가능\n",
    "\n",
    "- 특징의 유형에 상관없이 잘 동작\n",
    "\n",
    "- 특이점(Outlier)에 대하여 상대적으로 덜 민감\n",
    "\n",
    "- 튜닝이 필요한 파라미터의 개수가 작음\n",
    "\n",
    "- 값이 빠진 경우도 효율적으로 처리가 가능\n",
    "\n",
    "- 해석이 쉬워짐"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
