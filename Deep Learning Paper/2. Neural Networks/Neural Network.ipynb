{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - 생물학적 신경망의 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사람이 학습하는 방식을 비슷하게 구현한 것이 인공신경망이며, 이는 상당한 역사를 갖고 있다. <br />\n",
    "인공신경망의 구조를 알아보기에 앞서 ‘생물학적 신경망’을 먼저 살펴보면 다음 그림과 같다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/1.png\" width=500 />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경 세포(뉴런)의 구성은 크게 다음 4가지로 구성이 된다.\n",
    "\n",
    "- 수상돌기 (Dendrite): 외부로부터의 신경 자극을 받아들이는 역할을 한다.\n",
    "\n",
    "- 축삭돌기 (Axon): 가늘고 길게 뻗어 있는 섬유 모양을 하고 있으며, 전류와 비슷한 형태로 다른 뉴런으로 신호를 전달하는 기능을 담당한다.\n",
    "\n",
    "- 신경세포체 (Soma): 신경 세포의 핵을 담당하는 부분으로 여러 뉴런으로부터 전달되는 외부 자극에 대한 판정을 하여 다른 뉴런으로 신호를 전달할 것인지를 최종 결정을 한다.\n",
    "\n",
    "- 시냅스 (Synapse): 어떤 뉴런의 축삭돌기 말단과 다음 뉴런의 수상돌기의 연결 부위를 말한다. 이 시냅스는 얇은 막의 형태이며, 다른 뉴런의 축삭돌기로부터 받는 신호를 <br />\n",
    "어느 정도의 세기(strength, weight)로 전달할 것인지를 결정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network - 인공 신경망의 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습을 하게 되면, 한 신경 세포에서 다른 신경 세포로 연결되는 시냅스 부분에서 신호의 세기가 결정이 되고 굳어지게 된다.\n",
    "즉, 특정 자극에 학습의 기대치대로 결과를 낼 수 있도록 각각의 시냅스의 세기가 결정이 된다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/2.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 그림에서 볼 수 있는 것처럼, 인공 신경망은 생물학적 뉴런의 구조를 모방하여 만들어졌다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수상돌기에 해당되는 입력에$(x1, x2, …, xn)$ 시냅스에 해당하는 입력의 가중치$(w1, w2, …, wn)$를 곱하고 이것들의 총합이 신경세포체(Soma) 부분으로 전달이 되면, <br />\n",
    "신경 세포체에서는 활성함수(activation function)에 따라 최종 출력 Y가 결정이 된다. <br />\n",
    "위 그림에서는 활성 함수가 특정 경계값(threshold)과 비교를 하여 같거나 크면 ‘+1’을 출력하고 작으면 ‘-1’을 출력한다. <br />\n",
    "인공신경망(ANN)은 보통 이런 뉴런들을 multi-layer로 구성을 하며, 뒤에서 설명하게 될 역전파(back-propagation) 알고리즘을 통해 신경망의 학습 결과가 기대치와 <br /> \n",
    "비슷한 결과를 낼 수 있도록 뉴런의 입력으로 들어오는 시냅스의 가중치를 계속 조절해가는 과정을 거친다. <br />\n",
    "이것을 훈련(training)이라고 하며, 훈련 데이터를 통한 반복 훈련을 통해 가중치$(w1, w2, …, wn)$의 최적값이 정해지게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 태동기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1943년, McCulloch와 Pitts는 산술연산과 논리연산을 수행할 수 있는 간단한 신경 망에 기초한 연산 모델(computational model)을 만들고 이것을 발표하였으며, <br />\n",
    "이 논문은 이후의 Neural Network에 많은 영향을 끼치게 된다. <br />\n",
    "1949년 심리학자인 Hebb은 “The Organization of Behavior”라는 책을 발표했으며, 뉴런의 시냅스에 기반한 학습의 법칙(learning law)을 발표하였고, <br />\n",
    "심리학적인 실험 결과를 이 학습의 법칙에 의해 설명하였다. 후에 이것은 Hebbs의 이름을 따 ‘Hebbian learning’이라고 불린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 황금기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1950년대와 60년대는 신경망의 황금기로 불린다.c1951년 Minsky는 Snark라는 이름을 갖는 neurocomputer를 개발했으며, 가중치(weight)을 자동으로 조절할 수가 있었다. <br /> \n",
    "하지만 실제로 구현이 되지는 못했다. 1957년 Rosenblatt는 신경망에서 흔히 사용되는 “perceptron”이라는 용어 및 알고리즘을 개발했고, 후에 Rosenblatt는 <br />\n",
    "perceptron에 기반한 최초의 성공적인 Neuro-Computer를 개발했으며, 이것을 패턴 인식 분야에 적용하였다. <br />\n",
    "이 perceptron이 많은 일을 할 수 있을 것으로 전망이 되었지만, 얼마 안돼 제한이 많이 있음이 밝혀졌다. Single layer perceptron는 선형적으로 분리가 가능한 패턴은 <br /> \n",
    "인식할 수 있지만, 복잡한 패턴은 2개 혹은 그 이상의 layer를 갖는 신경망(multi-layer) 신경망에서 가능하다는 것이 1969년 Minsky에 의해 증명이 된다. <br />\n",
    "오늘날 perceptron은 한 쪽 혹은 다른 쪽에 속하는 것을 결정할 수 있는 binary classifier에 대한 지도 학습 알고리즘의 개념으로 사용이 되고 있다. <br />\n",
    "또한 Minsky는 perceptron으로는 XOR을 학습할 수 없다는 XOR problem을 밝히고 결국 이것은 신경망에 대한 관심이 멀어지게 만드는 계기가 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 긴 침묵기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1970년대에 들어서면서 연구비에 대한 지원이 줄고, 학회도 거의 없어지면서 논문이 출간되는 횟수도 크게 줄어들게 된다. <br />\n",
    "하지만, 개별적으로 신경망에 대한 연구는 지속이 되었으며, 독자적으로 신경망에 대한 패러다임을 발전시켰다. <br />\n",
    "그리고 이 시기의 연구들은 80년대 후반 르네상스 시대의 밑거름이 된다. <br />\n",
    "1976년 Grossberg는 많은 논문을 발표했으며, 그의 연구는 후에 Carpenter에 의해 ART(Adaptive resonance theory)로 발전을 하게 된다. <br />\n",
    "1982년 Kohonen은 Kohonen map이라고도 알려진 SOM(self-organization feature map)을 발표했다. <br />\n",
    "또한 Hopfield는 Hopfield 망을 발표하였다. <br />\n",
    "후에 SOM과 ART를 통하여 신경망을 자율학습(unsupervised learning) 분야에 적용을 할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 르네상스기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1982년과 1984년에 저명한 물리학자 Hopfield는 널리 읽히는 신경망 관련 논문 2편을 발표하였으며, 전세계를 돌면서 강의를 통해 신경망에 대한 연구를 활성화 시키기를 촉구한다. <br />\n",
    "이는 많은 연구자들이 다시 신경망에 관심을 갖게 되는 계기가 되었다. <br />\n",
    "1986년, Rumelhart와 Hinton이 드디어 그 유명한 back-propagation 알고리즘을 발표하면서 신경망은 많은 문제들을 풀 수 있게 되었다. <br />\n",
    "1987년에는 최초로 신경망에 대한 국제 conference가 열리게 되고, 그 다음 해에는 신경망에 대한 국제 journal이 만들어지게 되었다. <br />\n",
    "1995년 LeCun과 Bengio는 CNN(Convolutional Neural Network)를 발표한다. <br />\n",
    "CNN을 이용하여 local invariant feature를 쉽게 추출이 가능하고, 기존 신경망이 갖는 문제점을 극복할 수 있게 되었으며, 문자 인식이나 음성 인식 분야에서 <br /> 탁월한 성능을 얻을 수 있게 되었다. <br />\n",
    "하지만 신경망이 갖는 복잡도나 적절한 hyper-parameter 설정이 없이는 좋은 결과를 기대할 수도 없고, 좋은 hyper-parameter 설정을 경험적인 방법에 <br />\n",
    "많이 의지해야 하는 어려움이 있었다. <br />\n",
    "이로 인해 기계학습 분야에서 SVM(Support Vector Machine)이나 이것보다 훨씬 간단한 linear classifier 같은 알고리즘에 신경망이 점차 뒤로 밀리는 분위기가 되었다. <br />\n",
    "하지만, 2000년대 후반부터 부각된 딥러닝(deep learning) 때문에 다시 한번 신경망 연구에 불이 붙게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Hebbian Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1949년 심리학자인 Donald Hebb은 뉴런의 시냅스에 기반한 학습의 법칙을 발표하였다. <br />\n",
    "그는 생물학적인 신경망에서 학습이 이루어지면 특정 입력으로 들어오는 신호 자극에 잘 반응할 수 있도록 시냅스들의 세기가 결정이 된다는 사실에 주목했다.\n",
    "그의 이름을 딴 Hebbian rule에 따르면, 학습이란 시냅스 연결의 세기(strength)를 조정하는 것으로 정의했으며, 기본적인 학습 방법은 2개의 뉴런이 동시에 활성화 시키려면, <br /> \n",
    "뉴런이 연결된 가중치(weight)를 높이면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron의 개념\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/3.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1957년에 Rosenbalt는 “Perceptron”이라는 용어 및 개념을 발표했다. <br />\n",
    "발표 당시, 뉴런의 활성함수(activation function)로 “step function”을 사용했기 때문에 지금처럼 “sigmoid function”을 사용하는 뉴런에 비해 제약이 많았지만, <br />\n",
    "입력의 중요도에 따라 출력이 결정이 되는 수학적 모델로서는 의미가 있다. <br />\n",
    "여기서 입력의 중요도는 가중치에 따라 결정된다는 개념이 도입이 되었다. <br />\n",
    "2 layer feed forward 구조에서는 여러 개의 입력을 받아 1개의 출력을 결정하는 신경망의 경우를 살펴보면, 출력은 가중치와 입력의 곱의 합이 특정 기준(threshold)보다 <br /> \n",
    "작으면 0이 되고, 크면 1을 출력한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptron의 한계\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/4.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rosenbalt의 Perceptron은 그 개념상 생물학적인 신경망의 특성을 잘 반영한 것 같지만, 한계가 있다. <br />\n",
    "Perceptron의 문제는 뉴런의 활성화 함수가 step function이기 때문에, 출력이 0과 1처럼 극단적인 결과만을 도출할 수 있다. <br />\n",
    "2 layer 만으로 구성이 되거나, 아주 단순한 결과를 도출해야 하는 경우는 문제가 없지만, 뉴런의 출력이 0과 1로 극단적인 상황만 있기 때문에 다중 layer 신경망의 경우는 <br />\n",
    "좋은 결과를 얻기가 어렵다. <br />\n",
    "최적의 학습 결과를 갖는 신경망을 설계하려면, 역전파(back-propagation)와 gradient-descent 방법을 사용한다. <br />\n",
    "이 개념은 근본적으로 입력이나 특정 넷의 가중치를 약간 변경시키면, 출력에 작은 변화가 일어난다는 점에 근거하고 있다. <br />\n",
    "Perceptron 기반의 뉴런은 weight나 bias의 작은 변화가 출력 쪽에 작은 변화를 만들어내면서 섬세하게 신경망을 학습을 시킨다는 오늘날의 학습 개념과는 부합이 되지 않는다.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Images/5.png\" width=500 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
